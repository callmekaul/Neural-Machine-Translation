{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune(HI-GU)",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XNvBiOXMP0jZz88f2me00GWEEgS4ccwS",
      "authorship_tag": "ABX9TyM5b6xyWhxCDTKsSvBKc3XZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLfx2cobDIYN"
      },
      "source": [
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGoqKPZ4YJDe",
        "outputId": "19ace5f8-9736-4516-ff46-9fd4d9a1dbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "!pip install inltk\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import inltk\n",
        "from inltk.inltk import tokenize\n",
        "from inltk.inltk import setup\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: inltk in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: async-timeout>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from inltk) (3.13)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from inltk) (3.7.4.3)\n",
            "Requirement already satisfied: aiohttp>=3.5.4 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.6.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from inltk) (2.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from inltk) (4.6.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from inltk) (0.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from inltk) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from inltk) (1.4.1)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from inltk) (2.2.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from inltk) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from inltk) (20.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from inltk) (7.0.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from inltk) (1.3.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from inltk) (0.1.91)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from inltk) (7.352.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from inltk) (0.7)\n",
            "Requirement already satisfied: fastai==1.0.57 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.57)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (19.3.0)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (4.7.6)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.7.4.2)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (49.1.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.7.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (3.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (0.6.1+cu101)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2018.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (1.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.1.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2UbHQwXYTgt"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "#initialize Lang Class\n",
        "class Lang:\n",
        "   def __init__(self):\n",
        "       #initialize containers to hold the words and corresponding index\n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "       self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "#split a sentence into words and add it to the container\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "#If the word is not in the container, the word will be added to it, \n",
        "#else, update the word counter\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJQo1m3cYa27"
      },
      "source": [
        "def process_data():\n",
        "\n",
        "  sf = open('/content/drive/My Drive/data/bible-uedin.gu-hi.hi' , \"r\")\n",
        "  tf = open('/content/drive/My Drive/data/bible-uedin.gu-hi.gu' , \"r\")\n",
        "\n",
        "  source = Lang()\n",
        "  target = Lang()\n",
        "  pairs = []\n",
        "  count = 0\n",
        "  count2 = 0\n",
        "  for sent in sf:\n",
        "    sent = sent.strip()\n",
        "    source.addSentence(sent)\n",
        "    pairs.append(sent)\n",
        "    count = count+1\n",
        "\n",
        "  # print(count)\n",
        "\n",
        "  for sent in tf:\n",
        "    sent = sent.strip()\n",
        "    target.addSentence(sent.strip())\n",
        "    pairs.append(sent)\n",
        "    count2 = count2 + 1\n",
        "\n",
        "  # print(count2)\n",
        "\n",
        "  pairs_new = []\n",
        "\n",
        "  for i in range(count):\n",
        "    full = [pairs[i],pairs[i+count]]\n",
        "    pairs_new.append(full)\n",
        "\n",
        "\n",
        "\n",
        "  return source, target, pairs_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92bJScQFZ3Kz"
      },
      "source": [
        "# source, target, pairs = process_data()   JUST A CHECK TO SEE IF THE SIZE OF BOTH DATASET IS SAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRftbm5OC9Mp"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBdBN2bxdbC"
      },
      "source": [
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gUpG5q-DDop"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOZOdZTjdAWa"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PYQnQWvE0w5"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4xdfq9OdXqG"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tFsKzhQda3T"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBjkaSjjdgDg"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        # plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "    #     if iter % plot_every == 0:\n",
        "    #         plot_loss_avg = plot_loss_total / plot_every\n",
        "    #         plot_losses.append(plot_loss_avg)\n",
        "    #         plot_loss_total = 0\n",
        "\n",
        "    # showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLAP-AQedj8D"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(val_pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNDMkSjdx42",
        "outputId": "0bf823d8-ef57-4296-c09e-13dd02571de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "input_lang, output_lang, pairs = process_data()\n",
        "pairs = filterPairs(pairs)\n",
        "\n",
        "print(random.choice(pairs))\n",
        "\n",
        "print(len(pairs))\n",
        "x = len(pairs)\n",
        "\n",
        "trainSplit = int(x*0.7)\n",
        "valSplit = trainSplit + int(x*0.2)\n",
        "\n",
        "train_pairs = pairs[:trainSplit]\n",
        "val_pairs = pairs[trainSplit:valSplit]\n",
        "test_pairs = pairs[valSplit:]\n",
        "\n",
        "print(len(train_pairs))\n",
        "print(len(val_pairs))\n",
        "print(len(test_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['मनुष्य तो अपने से किसी बड़े की शपथ खाया करते हैं और उन के हर एक विवाद का फैसला शपथ से पक्का होता है।', 'માણસ પોતાના કરતાં મહાન વ્યક્તિના નામે શપથ લે છે. અને શપથથી સઘળી તકરારોનો અંત આવે છે.']\n",
            "7793\n",
            "5455\n",
            "1558\n",
            "780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F5bEVeZzcXW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwctMktLdqQw",
        "outputId": "5e5041e2-6bd1-4fcc-d476-780bc62ecfa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 30000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 14s (- 64m 59s) (1000 3%) 6.0378\n",
            "4m 23s (- 61m 29s) (2000 6%) 5.8585\n",
            "6m 36s (- 59m 32s) (3000 10%) 5.8957\n",
            "8m 50s (- 57m 30s) (4000 13%) 5.8440\n",
            "11m 2s (- 55m 10s) (5000 16%) 5.5966\n",
            "13m 15s (- 53m 3s) (6000 20%) 5.6115\n",
            "15m 29s (- 50m 53s) (7000 23%) 5.5255\n",
            "17m 40s (- 48m 35s) (8000 26%) 5.4592\n",
            "19m 48s (- 46m 12s) (9000 30%) 5.2485\n",
            "22m 5s (- 44m 10s) (10000 33%) 5.3426\n",
            "24m 18s (- 41m 59s) (11000 36%) 5.1443\n",
            "26m 37s (- 39m 55s) (12000 40%) 5.2694\n",
            "28m 50s (- 37m 42s) (13000 43%) 5.0930\n",
            "31m 3s (- 35m 29s) (14000 46%) 5.0825\n",
            "33m 19s (- 33m 19s) (15000 50%) 5.0749\n",
            "35m 31s (- 31m 5s) (16000 53%) 5.0240\n",
            "37m 47s (- 28m 53s) (17000 56%) 5.0407\n",
            "40m 1s (- 26m 41s) (18000 60%) 4.9178\n",
            "42m 15s (- 24m 27s) (19000 63%) 4.9367\n",
            "44m 29s (- 22m 14s) (20000 66%) 4.8658\n",
            "46m 47s (- 20m 3s) (21000 70%) 4.8057\n",
            "49m 1s (- 17m 49s) (22000 73%) 4.7698\n",
            "51m 19s (- 15m 37s) (23000 76%) 4.8196\n",
            "53m 35s (- 13m 23s) (24000 80%) 4.7339\n",
            "55m 52s (- 11m 10s) (25000 83%) 4.8177\n",
            "58m 7s (- 8m 56s) (26000 86%) 4.6627\n",
            "60m 24s (- 6m 42s) (27000 90%) 4.7032\n",
            "62m 39s (- 4m 28s) (28000 93%) 4.7660\n",
            "64m 56s (- 2m 14s) (29000 96%) 4.7207\n",
            "67m 12s (- 0m 0s) (30000 100%) 4.5630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLSoH_89d5Eg",
        "outputId": "eb0fd421-c52f-4b0d-f9c6-7fa58bf52e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> क्योंकि वह मूसा से इतना बढ़कर महिमा के योग्य समझा गया है, जितना कि घर बनानेवाला घर से बढ़कर आदर रखता है।\n",
            "= જ્યારે મનુષ્ય મકાન બાંધે છે, ત્યારે લોકો મકાન બાંધનારને ખુબ માન આપે છે. તેમ ઈસુ મૂસા કરતાં માન આપવાને વધુ યોગ્ય ઠર્યો.\n",
            "< અને જ્યારે પોતાના પોતાના અને ત્યારે અને અને તે છે. <EOS>\n",
            "\n",
            "> क्योंकि यदि वह पहिली वाचा निर्दोष होती, तो दूसरी के लिये अवसर न ढूंढ़ा जाता।\n",
            "= જો પ્રથમ કરાર દોષ વગરનો હોત તો, બીજા કરારની કોઈ જ જરુંરિયાત ન રહેત.\n",
            "< પરંતુ મારા જે કે છે હું છું તો ન છું તે હું છું તો તમને તો તો તો <EOS>\n",
            "\n",
            "> उसी में जिस में हम भी उसी की मनसा से जो अपनी इच्छा के मत के अनुसार सब कुछ करता है, पहिले से ठहराए जाकर मीरास बने।\n",
            "= ખ્રિસ્તમાં આપણે દેવના લોકો તરીકે પસંદ કરાયા. દેવે આપણને તેના વારસો બનાવવાનું આયોજન ક્યારનું ય કર્યુ હતું. કારણ કે દેવ એ જ ઈચ્છતો હતો. અને દેવ એક છે જે ઈચ્છે છે અને માંગે છે તેને અનુરૂપ બધી વસ્તુઓને કરી શકે છે.\n",
            "< જો તમે આમ ન ન ન ન ન ન ન ન હશે તો તે જ્યારે તમને આ ન તો ત્યારે તે <EOS>\n",
            "\n",
            "> इसलिये मैं ने उसे भेजने का और भी यत्न किया कि तुम उस से फिर भेंट करके आनन्दित हो जाओ और मेरा शोक घट जाए।\n",
            "= તેથી તેને મોકલવાની મારી ઘણી ઈચ્છા છે. જ્યારે તમે તેને જોશો, ત્યારે તમે આનંદીત થશો. અને મને તમારી ચિંતા નહિ થાય.\n",
            "< હું હું મારી મારી હું હું હું હું હું હું હું હું હું હું હું હું હું હું હું હું હું પણ હું પણ હું હું પણ હું હું હું હું પણ હું પણ હું હું પણ હું પણ હું પણ હું પણ હું પણ મારી મારી મારી <EOS>\n",
            "\n",
            "> इसलिये ध्यान से देखो, कि कैसी चाल चलते हो; निर्बुद्धियों की नाईं नहीं पर बुद्धिमानों की नाईं चलो।\n",
            "= તેથી તમે કેવી રીતે જીવો છો તે વિષે ખૂબ જ ચોક્કસ બનો, અને નિર્બુદ્ધ લોકો જેવું જીવન ના જીવો પરંતુ તે લોકોના જેવું જીવન જીવો જે ડાક્યા છે.\n",
            "< અને જે રીતે કે જે તે છે તે જે તે તે <EOS>\n",
            "\n",
            "> हम राज दिन बहुत ही प्रार्थना करते रहते हैं, कि तुम्हारा मुंह देखें, और तुम्हारे विश्वास की घटी पूरी करें।।\n",
            "= દિવસ અને રાત્રે તમારા માટે અતિશય પ્રાર્થના કરી રહ્યાં છીએ. અમે પ્રાર્થી રહ્યાં છીએ કે તમારા વિશ્વાસમાં જે કઈ ન્યૂનતા હોય તે સંપૂર્ણ કરવા અમે ત્યાં આવી શકીએ, તમને પુનઃમળી શકીએ અને તમને આવશ્યક બધી જ વસ્તુઓ તમને પૂરી પાડી શકીએ.\n",
            "< તેઓ તમે અને અને છે તે તે તે તે અને તે તે તે <EOS>\n",
            "\n",
            "> इस में परमेश्वर मेरा गवाह है, कि मैं मसीह यीशु की सी प्रीति करके तुम सब की लालसा करता हूं।\n",
            "= દેવ જાણે છે કે તમને મળવાને હું ઘણો આતુર છું. હું તમને બધાને ખ્રિસ્ત ઈસુના પ્રેમ સાથે ચાહું છું.\n",
            "< હું તમને સત્ય કહું છું છે જે તેના વિષે મેં જે કહ્યું છે તે તમને કહ્યું તે છે. <EOS>\n",
            "\n",
            "> बरन प्रेम में सच्चाई से चलते हुए, सब बातों में उस में जा सिर है, अर्थात् मसीह में बढ़ते जाएं।\n",
            "= ના! આપણે પ્રેમથી સત્ય બોલીશું. અને દરેક રીતે ખ્રિસ્ત જેવા બનવા આપણે વિકાસ કરીશું. ખ્રિસ્ત શિર છે અને આપણે શરીર છીએ.\n",
            "< પરંતુ તમે દીકરો જ છો, છું. પરંતુ તે તમે તે અને તે <EOS>\n",
            "\n",
            "> और ऐसी खराई पाई जाए, कि कोई उसे बुरा न कह सके; जिस से विरोधी हम पर कोई दोष लगाने की गौं न पाकर लज्जित हों।\n",
            "= અને જ્યારે તું બોલે, ત્યારે સત્ય જ ઉચ્ચારજે જેથી કરીને તારી ટીકા ન થાય. તે પછી તો તારો દુશ્મન શરમાઈ જશે કેમ કે આપણી વિરૂદ્ધ ખરાબ કહેવાનું એની પાસે કંઈ પણ હશે નહિ.\n",
            "< હવે હું હંમેશા ના હોય તો તે પણ ના હોય તે પણ નથી. તે પણ તમને ન વિશ્વાસ ન શકે નહિ. <EOS>\n",
            "\n",
            "> क्योंकि जब वह जो घटता जाता था तेजोमय था, तो वह जो स्थिर रहेगा, और भी तेजोमय क्यों न होगा?\n",
            "= મહિમા સાથે આવેલી જે સેવા અદશ્ય થવાની હતી, પછી તો આ સેવા જે અવિનાશી છે તેનો મહિમા વિશેષ છે.\n",
            "< કોઈ પણ માણસ છે કે છે જ્યારે ત્યારે ત્યારે ત્યારે ત્યારે તે તે <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLvGEhf0j0C1"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
        "       super().__init__()\n",
        "      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "#encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "#use the encoder’s hidden layer as the decoder hidden\n",
        "       decoder_hidden = encoder_hidden.to(device)\n",
        "  \n",
        "#add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "\n",
        "#topk is used to get the top K value over a list\n",
        "#predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == EOS_token):\n",
        "               break\n",
        "\n",
        "       return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piwtmuh7jDWh"
      },
      "source": [
        "model = Seq2Seq(encoder1, attn_decoder1, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3a93SLXj2gD",
        "outputId": "71c809b5-6594-4936-8082-f8fe7cad3e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, params in model.named_children():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder\n",
            "decoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5faGpUDKlZwZ"
      },
      "source": [
        "for param in model.parameters():    \n",
        "    param.requires_grad = False\n",
        "\n",
        "trained_encoder = list(model.children())[0]\n",
        "trained_decoder = list(model.children())[1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6vf0E0ScfsV",
        "outputId": "60260715-0c9c-419c-c6b8-11efb2f9cc0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder2 = trained_decoder\n",
        "\n",
        "trainIters(encoder2, attn_decoder2, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 16s (- 11m 31s) (1000 10%) 4.9451\n",
            "2m 31s (- 10m 7s) (2000 20%) 4.7210\n",
            "3m 47s (- 8m 49s) (3000 30%) 4.6865\n",
            "5m 1s (- 7m 32s) (4000 40%) 4.6307\n",
            "6m 17s (- 6m 17s) (5000 50%) 4.6512\n",
            "7m 33s (- 5m 2s) (6000 60%) 4.7900\n",
            "8m 48s (- 3m 46s) (7000 70%) 4.6817\n",
            "10m 5s (- 2m 31s) (8000 80%) 4.6622\n",
            "11m 22s (- 1m 15s) (9000 90%) 4.6944\n",
            "12m 38s (- 0m 0s) (10000 100%) 4.5908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul6xiMFA6YA6",
        "outputId": "738fc67a-455e-4712-bd23-47cede0bdd8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "new_model = Seq2Seq(encoder2, attn_decoder2, device).to(device)\n",
        "\n",
        "for param in new_model.parameters():    \n",
        "    param.requires_grad = True\n",
        "\n",
        "trained_encoder = list(new_model.children())[0]\n",
        "trained_decoder = list(new_model.children())[1] \n",
        "\n",
        "trainIters(trained_encoder, trained_decoder, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 17s (- 9m 9s) (1000 20%) 4.8457\n",
            "4m 35s (- 6m 52s) (2000 40%) 4.7291\n",
            "6m 51s (- 4m 34s) (3000 60%) 4.6403\n",
            "9m 7s (- 2m 16s) (4000 80%) 4.7279\n",
            "11m 24s (- 0m 0s) (5000 100%) 4.6831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WCaEOcIuVmV"
      },
      "source": [
        "def BLEU_score(encoder, decoder, n = len(test_pairs)):\n",
        "  score = 0\n",
        "  for i in range(n):\n",
        "        pair = random.choice(test_pairs)\n",
        "        reference = [pair[1].split(' ')]\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        candidate = output_sentence.split(' ')\n",
        "        i_score = sentence_bleu(reference, candidate)\n",
        "        score = score + i_score\n",
        "  avg_score = score/n\n",
        "\n",
        "  return(avg_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGLhsOdu3tb9",
        "outputId": "faa4d114-239b-48aa-8364-b3e5f93abc76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "base_score = BLEU_score(encoder1,attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lT4QRK84-kd",
        "outputId": "86fc9a65-c7d2-439b-d8b6-db656dc5ed86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tuned_score = BLEU_score(trained_encoder,trained_decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imhX4a8RgcRF",
        "outputId": "1b834446-3625-469f-b208-803cc46cd9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Number of Training Pairs : \", len(train_pairs))\n",
        "print(\"Number of Validation Pairs : \", len(val_pairs))\n",
        "print(\"Number of Test Pairs : \", len(test_pairs))\n",
        "print(\"Base Model Score : \", base_score*100)\n",
        "print(\"Tuned Model Score : \", tuned_score*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Pairs :  5455\n",
            "Number of Validation Pairs :  1558\n",
            "Number of Test Pairs :  780\n",
            "Base Model Score :  26.776900253341175\n",
            "Tuned Model Score :  27.983505591017828\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}