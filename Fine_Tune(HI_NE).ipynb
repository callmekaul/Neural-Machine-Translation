{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune(HI-NE)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLfx2cobDIYN"
      },
      "source": [
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGoqKPZ4YJDe",
        "outputId": "c63dfb82-0002-412c-b09c-2db19c881ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install inltk\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import inltk\n",
        "from inltk.inltk import tokenize\n",
        "from inltk.inltk import setup\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting inltk\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/6b/27e71de5a61d1cb3b37abb566880b023029b95183224a60088da9abb224b/inltk-0.8.1-py3-none-any.whl\n",
            "Collecting typing\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from inltk) (0.7)\n",
            "Collecting aiohttp>=3.5.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.2MB/s \n",
            "\u001b[?25hCollecting fastai==1.0.57\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/e2/42342ded0385d694e3250e74f43f0dc9a3ff3d5c2241a2ddd98236b5f9de/fastai-1.0.57-py3-none-any.whl (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from inltk) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from inltk) (1.4.1)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from inltk) (2.2.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from inltk) (2.23.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from inltk) (7.352.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from inltk) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from inltk) (20.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from inltk) (3.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from inltk) (4.6.3)\n",
            "Collecting async-timeout>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from inltk) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.18.5)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from inltk) (0.2.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from inltk) (7.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (19.3.0)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.7.4.2)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 40.7MB/s \n",
            "\u001b[?25hCollecting multidict<5.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.0.4)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (0.6.1+cu101)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (49.1.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.7.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (1.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.1.0)\n",
            "Building wheels for collected packages: typing, idna-ssl\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-cp36-none-any.whl size=26309 sha256=ce050c6eb177bf5f94eddcfc833bd28b9e4ec2f456cc51f4fde74b0bbf4d930e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3162 sha256=2237041b04f9a8d89730f0d5f35120ae45a7affd91a268251b027540d2cf81ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built typing idna-ssl\n",
            "Installing collected packages: typing, idna-ssl, async-timeout, multidict, yarl, aiohttp, fastai, sentencepiece, inltk\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed aiohttp-3.6.2 async-timeout-3.0.1 fastai-1.0.57 idna-ssl-1.1.0 inltk-0.8.1 multidict-4.7.6 sentencepiece-0.1.91 typing-3.7.4.3 yarl-1.4.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2UbHQwXYTgt"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 2000\n",
        "\n",
        "#initialize Lang Class\n",
        "class Lang:\n",
        "   def __init__(self):\n",
        "       #initialize containers to hold the words and corresponding index\n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "       self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "#split a sentence into words and add it to the container\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "#If the word is not in the container, the word will be added to it, \n",
        "#else, update the word counter\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJQo1m3cYa27"
      },
      "source": [
        "def process_data():\n",
        "\n",
        "  sf = open('/content/drive/My Drive/data/bible-uedin.hi-ne.hi' , \"r\")\n",
        "  tf = open('/content/drive/My Drive/data/bible-uedin.hi-ne.ne' , \"r\")\n",
        "\n",
        "  source = Lang()\n",
        "  target = Lang()\n",
        "  pairs = []\n",
        "  count = 0\n",
        "  count2 = 0\n",
        "  for sent in sf:\n",
        "    sent = sent.strip()\n",
        "    source.addSentence(sent)\n",
        "    pairs.append(sent)\n",
        "    count = count+1\n",
        "\n",
        "  # print(count)\n",
        "\n",
        "  for sent in tf:\n",
        "    sent = sent.strip()\n",
        "    target.addSentence(sent.strip())\n",
        "    pairs.append(sent)\n",
        "    count2 = count2 + 1\n",
        "\n",
        "  # print(count2)\n",
        "\n",
        "  pairs_new = []\n",
        "\n",
        "  for i in range(count):\n",
        "    full = [pairs[i],pairs[i+count]]\n",
        "    pairs_new.append(full)\n",
        "\n",
        "\n",
        "\n",
        "  return source, target, pairs_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92bJScQFZ3Kz"
      },
      "source": [
        "# source, target, pairs = process_data()   JUST A CHECK TO SEE IF THE SIZE OF BOTH DATASET IS SAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRftbm5OC9Mp"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBdBN2bxdbC"
      },
      "source": [
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gUpG5q-DDop"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOZOdZTjdAWa"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PYQnQWvE0w5"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4xdfq9OdXqG"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tFsKzhQda3T"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBjkaSjjdgDg"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        # plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "    #     if iter % plot_every == 0:\n",
        "    #         plot_loss_avg = plot_loss_total / plot_every\n",
        "    #         plot_losses.append(plot_loss_avg)\n",
        "    #         plot_loss_total = 0\n",
        "\n",
        "    # showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLAP-AQedj8D"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(val_pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNDMkSjdx42",
        "outputId": "8c7577b0-d3f2-4584-e653-113f307a0623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "input_lang, output_lang, pairs = process_data()\n",
        "pairs = filterPairs(pairs)\n",
        "\n",
        "print(random.choice(pairs))\n",
        "\n",
        "print(len(pairs))\n",
        "x = len(pairs)\n",
        "\n",
        "trainSplit = int(x*0.7)\n",
        "valSplit = trainSplit + int(x*0.2)\n",
        "\n",
        "train_pairs = pairs[:trainSplit]\n",
        "val_pairs = pairs[trainSplit:valSplit]\n",
        "test_pairs = pairs[valSplit:]\n",
        "\n",
        "print(len(train_pairs))\n",
        "print(len(val_pairs))\n",
        "print(len(test_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['मैं तुम को इस में से निकालकर परदेशियों के हाथ में कर दूंगा, और तुम को दण्ड दिलाऊंगा।', 'परमेश्वरले यो पनि भन्नुभयो, “म तिमीहरूलाई यो शहरबाट बाहिर निकाल्नेछु अनि तिमीहरूलाई अपरिचितहरूको हातमा सुम्पिदिनेछु। म तिमीहरूलाई दण्ड दिनेछु!']\n",
            "30486\n",
            "21340\n",
            "6097\n",
            "3049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F5bEVeZzcXW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwctMktLdqQw",
        "outputId": "c25a2825-6dcd-4828-895b-f702376da3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 30000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 47s (- 80m 56s) (1000 3%) 6.5509\n",
            "5m 32s (- 77m 41s) (2000 6%) 6.4346\n",
            "8m 16s (- 74m 27s) (3000 10%) 6.2416\n",
            "10m 58s (- 71m 22s) (4000 13%) 6.1660\n",
            "13m 37s (- 68m 7s) (5000 16%) 6.2259\n",
            "16m 16s (- 65m 5s) (6000 20%) 6.1298\n",
            "19m 0s (- 62m 26s) (7000 23%) 6.1571\n",
            "21m 47s (- 59m 54s) (8000 26%) 6.1937\n",
            "24m 32s (- 57m 16s) (9000 30%) 6.1979\n",
            "27m 17s (- 54m 34s) (10000 33%) 6.0655\n",
            "30m 0s (- 51m 50s) (11000 36%) 6.0530\n",
            "32m 44s (- 49m 6s) (12000 40%) 6.0149\n",
            "35m 26s (- 46m 21s) (13000 43%) 6.0222\n",
            "38m 12s (- 43m 39s) (14000 46%) 5.9743\n",
            "40m 57s (- 40m 57s) (15000 50%) 6.1086\n",
            "43m 46s (- 38m 17s) (16000 53%) 6.1033\n",
            "46m 36s (- 35m 38s) (17000 56%) 6.1417\n",
            "49m 24s (- 32m 56s) (18000 60%) 5.9593\n",
            "52m 20s (- 30m 18s) (19000 63%) 6.0595\n",
            "55m 12s (- 27m 36s) (20000 66%) 5.9755\n",
            "58m 5s (- 24m 53s) (21000 70%) 6.0166\n",
            "61m 2s (- 22m 11s) (22000 73%) 6.0236\n",
            "63m 59s (- 19m 28s) (23000 76%) 6.0507\n",
            "66m 51s (- 16m 42s) (24000 80%) 5.9018\n",
            "69m 49s (- 13m 57s) (25000 83%) 5.9767\n",
            "72m 45s (- 11m 11s) (26000 86%) 5.8544\n",
            "75m 39s (- 8m 24s) (27000 90%) 5.9859\n",
            "78m 35s (- 5m 36s) (28000 93%) 5.8489\n",
            "81m 27s (- 2m 48s) (29000 96%) 5.8738\n",
            "84m 20s (- 0m 0s) (30000 100%) 5.7955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLSoH_89d5Eg",
        "outputId": "9267cf19-4035-4cf3-cbd4-1cd52d13f466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> और उन्हों ने पत्थर को कब्र पर से लुढ़का हुआ पाया।\n",
            "= चिहानको प्रवेशद्वार बन्द गर्नलाई एउटा ठूलो ढुङ्गा राखिएको थियो। तर तिनीहरूले त्यो ढुङ्गा चिहानदेखि कतै गुड्याइ पठाएको पाए।\n",
            "< त्यसपछि मानिसहरू मानिसहरू अनि अनि अनि अनि मानिसहरू अनि अनि अनि <EOS>\n",
            "\n",
            "> पर मैं तुम से कहता हूं, कि न्याय के दिन तेरी दशा से सदोम के देश की दशा अधिक सहने योग्य होगी।\n",
            "= तर तिमीलाई भन्छु कि न्यायको दिनमा सदोमको भन्दा तिम्रो हालत अरू नराम्रो हुनेछ।”\n",
            "< म तिम्रो म म म अनि म म अनि म अनि म अनि म अनि म अनि म अनि म <EOS>\n",
            "\n",
            "> और उस ने उन से कहा, कि जब मैं ने तुम्हें बटुए, और झोली, और जूते बिना भेजा था, तो क्या तुम को किसी वस्तु की घटी हुई थी? उन्हों ने कहा; किसी वस्तु की नहीं।\n",
            "= तब येशूले प्रेरितहरूलाई भन्नुभयो, “जब मैले तिमीहरूले मानिसहरूमा प्रचार गर्न, विना पैसा, झोला अनि जुत्ताहरू विना पठाएँ, के तिमीहरूलाई केही अभाव भयो?” प्रेरितहरूले भने, “भएन।”\n",
            "< तर ती मानिसहरू भने, भने, मानिसहरू अनि म अनि अनि म अनि म अनि म अनि म अनि अनि म अनि अनि म अनि म अनि <EOS>\n",
            "\n",
            "> और जब वहां पहुंचे तो वे उस अटारी पर गए, जहां पतरस और यूहन्ना और याकूब और अन्द्रियास और फिलिप्पुस और थोमा और बरतुलमाई और मत्ती और हलफई का पुत्रा याकूब और शमौन जेलोतेस और याकूब का पुत्रा यहूदा रहते थे।\n",
            "= प्रेरितहरू शहरभित्र पसे। तिनीहरू त्यही ठाउँमा गए जहाँ तिनीहरू बसेका थिए; यो कोठा माथिल्लो तलामा थियो। प्रेरितहरू थिए पत्रुस, यूहन्ना, याकूब अन्द्रियास, फिलिप, थोमा, बर्थुलमै, मत्ती, याकूब (अल्फयसको छोरो) र शिमोन (कनानी) र यहूदा (याकूबको छोरो)।\n",
            "< तर ती अनि अनि अनि अनि अनि अनि अनि थिए। अनि <EOS>\n",
            "\n",
            "> हे प्रभु, सुन ले; हे प्रभु, पाप क्षमा कर; हे प्रभु, ध्यान देकर जो करता है उसे कर, विलम्ब न कर; हे मेरे परमेश्वर, तेरा नगर और तेरी प्रजा तेरी ही कहलाती है; इसलिये अपने नाम के निमित्त ऐसा ही कर।।\n",
            "= हे परमप्रभु! सुन्नुहोस्, मेरा परमेश्वर हाम्रो पाप क्षमा गर्नुहोस्, मेरो परमेश्वर हाम्रो प्रार्थना सुनेर त्यस अनुसार गरिदिनु होस। किनभने यो तपाईं आफ्नो भलाईको निम्ति गर्नुहोस, ढिलो नगर्नुहोस, तपाईंको शहर र तपाईंको नानिस तपाईंकै नाउँले कहलाइएकोछन्।”\n",
            "< हे परमप्रभु, म तपाईंको म म म म म म अनि म मलाई म म अनि म अनि म अनि म अनि म मलाई म अनि म अनि <EOS>\n",
            "\n",
            "> पुबलियुस का पिता ज्वर और आंव लोहू से रोगी पड़ा था: सो पौलुस ने उसके पास घर में जाकर प्रार्थना की, और उस पर हाथ रखकर उसे चंगा किया।\n",
            "= पब्लियसको बाबु बिमार थिए। उनी ज्वरो र रगत मासीले थलो परेका थिए। तर पावल उनीकहाँ गए अनि प्रार्थना गरे। प्रार्थना पछि उनले आफ्नो हातहरू उनीमाथि राखीदिए र निको पारे।\n",
            "< यहूदाका मानिसहरू मानिसहरू अनि मानिसहरू अनि अनि गए। गए। तिनीहरूले मानिसहरू अनि अनि अनि मानिसहरू अनि अनि <EOS>\n",
            "\n",
            "> फिर यीशु फतह से छ: दिन पहिले बैतनिरयाह में आया, जंहा लाजर था: जिसे यीशु ने मरे हुओं में से जिलाया था।\n",
            "= निस्तार चाड हुनु भन्दा छ दिन अघि येशू बेथानी तर्फ जानुभयो। बेथामी त्यो शहर थियो जहाँ लाजरस बस्थे। लाजरस येशूले मृत्यु बाट उठाएका मानिस हुन्।\n",
            "< त्यसपछि त्यस दिन वर्षको अनि त्यस दिन गए। त्यस दिन गए। <EOS>\n",
            "\n",
            "> और अपने अपने मन में एक दूसरे की हानि की कल्पना न करना, और झूठी शपथ से प्रीति न रखना, क्योंकि इन सब कामों से मैं धृणा करता हूं, यहोवा की यही वाणी है।।\n",
            "= तिम्रो छिमेकीलाई चोट पुर्याउने गुप्त योजनाहरू नबनाऊ। झुटो कसम नखाऊ। किन? किनकि म त्यो कुराहरूलाई घृणा गर्छु।” परमप्रभुले यसो भन्नु भयो।\n",
            "< म ती म अनि अनि अनि म अनि अनि म अनि अनि म अनि म अनि अनि म अनि म अनि म अनि अनि म अनि <EOS>\n",
            "\n",
            "> और यदि तुम मनुष्य के पुत्रा को जहां वह पहिले था, वहां ऊपर जाते देखोगे, तो क्या होगा?\n",
            "= तब तिमीहरूले मानिसको पुत्रलाई जहाँ ऊ पहिले थियो, त्यहाँ जाँदै गरेको देख्यौ भने के हुन्छ?\n",
            "< तर यदि त्यस मानिसलाई भने अनि अनि अनि अनि <EOS>\n",
            "\n",
            "> और वे उस की ताक में लगे और भेदिये भेजे, कि धर्म का भेष धरकर उस की कोई न कोई बात पकड़ें, कि उसे हाकिम के हाथ और अधिकार में सौंप दें।\n",
            "= यसपछि व्यवस्थाका शास्त्रीहरू र मुख्य पूजाहारीहरूले येशूलाई होसियारीसित चियो गरे। तिनीहरूले केही गुप्तचरहरू येशूकहाँ पठाए अनि तिनीहरूलाई असल मानिस झैं व्यवहार गर्ने भने। तिनीहरूले येशूको भनाईमा केही खोट निकाल्नु चहान्थे। यदि तिनीहरूले कुनै खोट पाए येशूलाई राज्यपाल अघि सुम्पिदिने थिए, जसको येशूमाथि शक्ति अनि अधिकार थियो।\n",
            "< तर ती मानिसहरूले अनि अनि अनि अनि अनि अनि तिनीहरू तिनीहरूले अनि अनि अनि <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLvGEhf0j0C1"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
        "       super().__init__()\n",
        "      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "#encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "#use the encoder’s hidden layer as the decoder hidden\n",
        "       decoder_hidden = encoder_hidden.to(device)\n",
        "  \n",
        "#add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "\n",
        "#topk is used to get the top K value over a list\n",
        "#predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == EOS_token):\n",
        "               break\n",
        "\n",
        "       return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piwtmuh7jDWh"
      },
      "source": [
        "model = Seq2Seq(encoder1, attn_decoder1, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3a93SLXj2gD",
        "outputId": "168c28b0-31e3-4484-b935-a0e35890bac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, params in model.named_children():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder\n",
            "decoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5faGpUDKlZwZ"
      },
      "source": [
        "for param in model.parameters():    \n",
        "    param.requires_grad = False\n",
        "\n",
        "trained_encoder = list(model.children())[0]\n",
        "trained_decoder = list(model.children())[1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6vf0E0ScfsV",
        "outputId": "995fa71b-56ff-4b6c-e7cb-b822f31ea9fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder2 = trained_decoder\n",
        "\n",
        "trainIters(encoder2, attn_decoder2, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 40s (- 23m 33s) (1000 6%) 6.3637\n",
            "3m 16s (- 21m 20s) (2000 13%) 6.0927\n",
            "4m 51s (- 19m 27s) (3000 20%) 6.0425\n",
            "6m 28s (- 17m 48s) (4000 26%) 6.1265\n",
            "8m 6s (- 16m 13s) (5000 33%) 6.0086\n",
            "9m 45s (- 14m 37s) (6000 40%) 6.0939\n",
            "11m 23s (- 13m 1s) (7000 46%) 6.0783\n",
            "13m 5s (- 11m 26s) (8000 53%) 6.0864\n",
            "14m 43s (- 9m 49s) (9000 60%) 6.0961\n",
            "16m 23s (- 8m 11s) (10000 66%) 6.1685\n",
            "18m 3s (- 6m 33s) (11000 73%) 6.1157\n",
            "19m 42s (- 4m 55s) (12000 80%) 6.0090\n",
            "21m 21s (- 3m 17s) (13000 86%) 5.9824\n",
            "23m 2s (- 1m 38s) (14000 93%) 6.0688\n",
            "24m 45s (- 0m 0s) (15000 100%) 6.1751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul6xiMFA6YA6",
        "outputId": "30c68565-4e4a-4daf-bf2a-f5d50a39e33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "new_model = Seq2Seq(encoder2, attn_decoder2, device).to(device)\n",
        "\n",
        "for param in new_model.parameters():    \n",
        "    param.requires_grad = True\n",
        "\n",
        "trained_encoder = list(new_model.children())[0]\n",
        "trained_decoder = list(new_model.children())[1] \n",
        "\n",
        "trainIters(trained_encoder, trained_decoder, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 58s (- 41m 44s) (1000 6%) 5.9711\n",
            "5m 51s (- 38m 2s) (2000 13%) 5.8885\n",
            "8m 51s (- 35m 25s) (3000 20%) 5.7722\n",
            "11m 49s (- 32m 31s) (4000 26%) 5.9127\n",
            "14m 47s (- 29m 34s) (5000 33%) 5.9158\n",
            "17m 40s (- 26m 31s) (6000 40%) 5.8018\n",
            "20m 38s (- 23m 35s) (7000 46%) 5.8331\n",
            "23m 34s (- 20m 37s) (8000 53%) 5.8553\n",
            "26m 35s (- 17m 43s) (9000 60%) 5.8881\n",
            "29m 36s (- 14m 48s) (10000 66%) 5.8838\n",
            "32m 32s (- 11m 50s) (11000 73%) 5.7826\n",
            "35m 32s (- 8m 53s) (12000 80%) 5.8217\n",
            "38m 27s (- 5m 55s) (13000 86%) 5.7463\n",
            "41m 28s (- 2m 57s) (14000 93%) 5.9020\n",
            "44m 22s (- 0m 0s) (15000 100%) 5.8471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WCaEOcIuVmV"
      },
      "source": [
        "def BLEU_score(encoder, decoder, n = len(test_pairs)):\n",
        "  score = 0\n",
        "  for i in range(n):\n",
        "        pair = random.choice(test_pairs)\n",
        "        reference = [pair[1].split(' ')]\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        candidate = output_sentence.split(' ')\n",
        "        i_score = sentence_bleu(reference, candidate)\n",
        "        score = score + i_score\n",
        "  avg_score = score/n\n",
        "\n",
        "  return(avg_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGLhsOdu3tb9",
        "outputId": "b7bb2829-393a-4bba-c077-eae0cfd5279c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "base_score = BLEU_score(encoder1,attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lT4QRK84-kd",
        "outputId": "34b3c60e-3405-40ac-ba14-b98e430d5123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tuned_score = BLEU_score(trained_encoder,trained_decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imhX4a8RgcRF",
        "outputId": "1197ed05-635e-495e-82dc-49e0e2140e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Number of Training Pairs : \", len(train_pairs))\n",
        "print(\"Number of Validation Pairs : \", len(val_pairs))\n",
        "print(\"Number of Test Pairs : \", len(test_pairs))\n",
        "print(\"Base Model Score : \", base_score*100)\n",
        "print(\"Tuned Model Score : \", tuned_score*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Pairs :  21340\n",
            "Number of Validation Pairs :  6097\n",
            "Number of Test Pairs :  3049\n",
            "Base Model Score :  22.937959302089144\n",
            "Tuned Model Score :  23.389528609976853\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}