{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINE_TUNE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNxGF/IP3mtKSQRM2ULhMe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y0cb-XFCWdK"
      },
      "source": [
        "Downloading spacy tokenizers for German, English and French"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qwFFBW7BdI6",
        "outputId": "dc8afa10-cf51-4e44-bbb7-201115d6c456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python -m spacy download de\n",
        "!python -m spacy download en\n",
        "!python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: fr_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz#egg=fr_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjaMPI-dCb4f"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GoZPx-WAYif"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.data.utils import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import collections\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z6W_HUoAhoI"
      },
      "source": [
        "# Set the random seed for reproducability\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvlCmSvbCfdY"
      },
      "source": [
        "Creating tokenizer instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "havC7zfyAjsF"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')\n",
        "spacy_fr = spacy.load('fr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg_bLoByAl-Z"
      },
      "source": [
        "def tokenize_de(text):\n",
        "\n",
        "    # Tokenizes German text from a string into a list of strings\n",
        "\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "\n",
        "    # Tokenizes English text from a string into a list of strings\n",
        "\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_fr(text):\n",
        "\n",
        "    # Tokenizes French text from a string into a list of strings\n",
        "\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsTRbU58Cqfv"
      },
      "source": [
        "Defining source language and target language fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHRE7k4EBrjp"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            include_lengths = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxjzwImjCwVZ"
      },
      "source": [
        "Splitting dataset into train, validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4gIAZSiBut5"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))\n",
        "\n",
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv0i1cygC1-C"
      },
      "source": [
        "Defining the Encoder, Attention Module, Decoder and the overarching Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-fDmsgBCG9S"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "                \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "                                 \n",
        "        #packed_outputs is a packed sequence containing all hidden states\n",
        "        #hidden is now from the final non-padded element in the batch\n",
        "            \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "        #outputs is now a non-packed sequence, all hidden states obtained\n",
        "        #  when the input is a pad token are all zeros\n",
        "            \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "  \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention = [batch size, src len]\n",
        "        \n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        return F.softmax(attention, dim = 1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #mask = [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erH1JDA1DUIf"
      },
      "source": [
        "Creating the base Seq2Seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utQYLxr1De_l"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "seq2seq = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIEgcGn4DoY4",
        "outputId": "f7f84bfb-3074-4735-9364-756aaf149f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Initializing the model parameters\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "seq2seq.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viSi4O0WEC4q",
        "outputId": "3efbe482-eae8-4b14-b674-714c645c05a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(seq2seq):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 20,518,917 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A7stlZWEFPJ"
      },
      "source": [
        "optimizer = optim.Adam(seq2seq.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhBiHIBwEShZ"
      },
      "source": [
        "Defining the Train and Evaluate methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkh1TgCEPMG"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-5AsyAMEelW"
      },
      "source": [
        "# Function to calculate Epoch Time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOlNB1CIEeQa"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfGDEneaEowm",
        "outputId": "70ae76cb-47f1-4b82-94a4-7f9369b2219d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(seq2seq, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(seq2seq, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(seq2seq.state_dict(), 'base-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 1m 35s\n",
            "\tTrain Loss: 5.045 | Train PPL: 155.267\n",
            "\t Val. Loss: 4.786 |  Val. PPL: 119.841\n",
            "Epoch: 02 | Time: 1m 35s\n",
            "\tTrain Loss: 4.130 | Train PPL:  62.203\n",
            "\t Val. Loss: 4.197 |  Val. PPL:  66.508\n",
            "Epoch: 03 | Time: 1m 35s\n",
            "\tTrain Loss: 3.381 | Train PPL:  29.408\n",
            "\t Val. Loss: 3.593 |  Val. PPL:  36.357\n",
            "Epoch: 04 | Time: 1m 35s\n",
            "\tTrain Loss: 2.875 | Train PPL:  17.733\n",
            "\t Val. Loss: 3.382 |  Val. PPL:  29.441\n",
            "Epoch: 05 | Time: 1m 34s\n",
            "\tTrain Loss: 2.490 | Train PPL:  12.061\n",
            "\t Val. Loss: 3.256 |  Val. PPL:  25.944\n",
            "Epoch: 06 | Time: 1m 35s\n",
            "\tTrain Loss: 2.206 | Train PPL:   9.082\n",
            "\t Val. Loss: 3.280 |  Val. PPL:  26.588\n",
            "Epoch: 07 | Time: 1m 34s\n",
            "\tTrain Loss: 1.960 | Train PPL:   7.096\n",
            "\t Val. Loss: 3.167 |  Val. PPL:  23.746\n",
            "Epoch: 08 | Time: 1m 35s\n",
            "\tTrain Loss: 1.769 | Train PPL:   5.863\n",
            "\t Val. Loss: 3.257 |  Val. PPL:  25.961\n",
            "Epoch: 09 | Time: 1m 35s\n",
            "\tTrain Loss: 1.614 | Train PPL:   5.021\n",
            "\t Val. Loss: 3.273 |  Val. PPL:  26.400\n",
            "Epoch: 10 | Time: 1m 35s\n",
            "\tTrain Loss: 1.502 | Train PPL:   4.492\n",
            "\t Val. Loss: 3.259 |  Val. PPL:  26.017\n",
            "Epoch: 11 | Time: 1m 35s\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.885\n",
            "\t Val. Loss: 3.434 |  Val. PPL:  31.014\n",
            "Epoch: 12 | Time: 1m 35s\n",
            "\tTrain Loss: 1.281 | Train PPL:   3.599\n",
            "\t Val. Loss: 3.461 |  Val. PPL:  31.845\n",
            "Epoch: 13 | Time: 1m 34s\n",
            "\tTrain Loss: 1.181 | Train PPL:   3.258\n",
            "\t Val. Loss: 3.477 |  Val. PPL:  32.378\n",
            "Epoch: 14 | Time: 1m 35s\n",
            "\tTrain Loss: 1.102 | Train PPL:   3.009\n",
            "\t Val. Loss: 3.669 |  Val. PPL:  39.229\n",
            "Epoch: 15 | Time: 1m 34s\n",
            "\tTrain Loss: 1.039 | Train PPL:   2.828\n",
            "\t Val. Loss: 3.539 |  Val. PPL:  34.430\n",
            "Epoch: 16 | Time: 1m 34s\n",
            "\tTrain Loss: 0.963 | Train PPL:   2.620\n",
            "\t Val. Loss: 3.705 |  Val. PPL:  40.630\n",
            "Epoch: 17 | Time: 1m 35s\n",
            "\tTrain Loss: 0.897 | Train PPL:   2.453\n",
            "\t Val. Loss: 3.684 |  Val. PPL:  39.801\n",
            "Epoch: 18 | Time: 1m 34s\n",
            "\tTrain Loss: 0.852 | Train PPL:   2.343\n",
            "\t Val. Loss: 3.782 |  Val. PPL:  43.925\n",
            "Epoch: 19 | Time: 1m 34s\n",
            "\tTrain Loss: 0.792 | Train PPL:   2.207\n",
            "\t Val. Loss: 3.836 |  Val. PPL:  46.363\n",
            "Epoch: 20 | Time: 1m 34s\n",
            "\tTrain Loss: 0.751 | Train PPL:   2.119\n",
            "\t Val. Loss: 3.906 |  Val. PPL:  49.709\n",
            "Epoch: 21 | Time: 1m 35s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.034\n",
            "\t Val. Loss: 3.928 |  Val. PPL:  50.816\n",
            "Epoch: 22 | Time: 1m 34s\n",
            "\tTrain Loss: 0.672 | Train PPL:   1.958\n",
            "\t Val. Loss: 4.078 |  Val. PPL:  59.048\n",
            "Epoch: 23 | Time: 1m 35s\n",
            "\tTrain Loss: 0.655 | Train PPL:   1.926\n",
            "\t Val. Loss: 4.078 |  Val. PPL:  59.015\n",
            "Epoch: 24 | Time: 1m 35s\n",
            "\tTrain Loss: 0.591 | Train PPL:   1.806\n",
            "\t Val. Loss: 4.242 |  Val. PPL:  69.563\n",
            "Epoch: 25 | Time: 1m 35s\n",
            "\tTrain Loss: 0.582 | Train PPL:   1.789\n",
            "\t Val. Loss: 4.280 |  Val. PPL:  72.228\n",
            "Epoch: 26 | Time: 1m 34s\n",
            "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
            "\t Val. Loss: 4.316 |  Val. PPL:  74.918\n",
            "Epoch: 27 | Time: 1m 35s\n",
            "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
            "\t Val. Loss: 4.380 |  Val. PPL:  79.805\n",
            "Epoch: 28 | Time: 1m 35s\n",
            "\tTrain Loss: 0.496 | Train PPL:   1.642\n",
            "\t Val. Loss: 4.502 |  Val. PPL:  90.227\n",
            "Epoch: 29 | Time: 1m 34s\n",
            "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
            "\t Val. Loss: 4.454 |  Val. PPL:  85.977\n",
            "Epoch: 30 | Time: 1m 34s\n",
            "\tTrain Loss: 0.455 | Train PPL:   1.577\n",
            "\t Val. Loss: 4.529 |  Val. PPL:  92.643\n",
            "Epoch: 31 | Time: 1m 35s\n",
            "\tTrain Loss: 0.457 | Train PPL:   1.579\n",
            "\t Val. Loss: 4.588 |  Val. PPL:  98.316\n",
            "Epoch: 32 | Time: 1m 34s\n",
            "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
            "\t Val. Loss: 4.681 |  Val. PPL: 107.869\n",
            "Epoch: 33 | Time: 1m 35s\n",
            "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
            "\t Val. Loss: 4.678 |  Val. PPL: 107.537\n",
            "Epoch: 34 | Time: 1m 34s\n",
            "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
            "\t Val. Loss: 4.670 |  Val. PPL: 106.696\n",
            "Epoch: 35 | Time: 1m 35s\n",
            "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
            "\t Val. Loss: 4.749 |  Val. PPL: 115.435\n",
            "Epoch: 36 | Time: 1m 34s\n",
            "\tTrain Loss: 0.373 | Train PPL:   1.451\n",
            "\t Val. Loss: 4.839 |  Val. PPL: 126.324\n",
            "Epoch: 37 | Time: 1m 35s\n",
            "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
            "\t Val. Loss: 4.855 |  Val. PPL: 128.389\n",
            "Epoch: 38 | Time: 1m 35s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
            "\t Val. Loss: 4.908 |  Val. PPL: 135.320\n",
            "Epoch: 39 | Time: 1m 34s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
            "\t Val. Loss: 4.859 |  Val. PPL: 128.948\n",
            "Epoch: 40 | Time: 1m 34s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
            "\t Val. Loss: 4.970 |  Val. PPL: 143.956\n",
            "Epoch: 41 | Time: 1m 34s\n",
            "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
            "\t Val. Loss: 5.002 |  Val. PPL: 148.641\n",
            "Epoch: 42 | Time: 1m 34s\n",
            "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
            "\t Val. Loss: 5.094 |  Val. PPL: 162.987\n",
            "Epoch: 43 | Time: 1m 34s\n",
            "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
            "\t Val. Loss: 5.128 |  Val. PPL: 168.758\n",
            "Epoch: 44 | Time: 1m 34s\n",
            "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
            "\t Val. Loss: 5.184 |  Val. PPL: 178.372\n",
            "Epoch: 45 | Time: 1m 35s\n",
            "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
            "\t Val. Loss: 5.180 |  Val. PPL: 177.771\n",
            "Epoch: 46 | Time: 1m 35s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
            "\t Val. Loss: 5.196 |  Val. PPL: 180.492\n",
            "Epoch: 47 | Time: 1m 35s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 5.263 |  Val. PPL: 193.078\n",
            "Epoch: 48 | Time: 1m 35s\n",
            "\tTrain Loss: 0.298 | Train PPL:   1.348\n",
            "\t Val. Loss: 5.289 |  Val. PPL: 198.133\n",
            "Epoch: 49 | Time: 1m 34s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
            "\t Val. Loss: 5.270 |  Val. PPL: 194.376\n",
            "Epoch: 50 | Time: 1m 34s\n",
            "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
            "\t Val. Loss: 5.304 |  Val. PPL: 201.069\n",
            "Epoch: 51 | Time: 1m 35s\n",
            "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
            "\t Val. Loss: 5.356 |  Val. PPL: 211.825\n",
            "Epoch: 52 | Time: 1m 34s\n",
            "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
            "\t Val. Loss: 5.366 |  Val. PPL: 214.027\n",
            "Epoch: 53 | Time: 1m 34s\n",
            "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
            "\t Val. Loss: 5.375 |  Val. PPL: 216.011\n",
            "Epoch: 54 | Time: 1m 35s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 5.403 |  Val. PPL: 222.053\n",
            "Epoch: 55 | Time: 1m 34s\n",
            "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
            "\t Val. Loss: 5.374 |  Val. PPL: 215.663\n",
            "Epoch: 56 | Time: 1m 34s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
            "\t Val. Loss: 5.425 |  Val. PPL: 226.945\n",
            "Epoch: 57 | Time: 1m 35s\n",
            "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
            "\t Val. Loss: 5.408 |  Val. PPL: 223.257\n",
            "Epoch: 58 | Time: 1m 35s\n",
            "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
            "\t Val. Loss: 5.535 |  Val. PPL: 253.307\n",
            "Epoch: 59 | Time: 1m 34s\n",
            "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
            "\t Val. Loss: 5.468 |  Val. PPL: 237.034\n",
            "Epoch: 60 | Time: 1m 34s\n",
            "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
            "\t Val. Loss: 5.565 |  Val. PPL: 261.106\n",
            "Epoch: 61 | Time: 1m 34s\n",
            "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
            "\t Val. Loss: 5.537 |  Val. PPL: 253.863\n",
            "Epoch: 62 | Time: 1m 34s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
            "\t Val. Loss: 5.621 |  Val. PPL: 276.046\n",
            "Epoch: 63 | Time: 1m 35s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 5.639 |  Val. PPL: 281.236\n",
            "Epoch: 64 | Time: 1m 34s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 5.654 |  Val. PPL: 285.517\n",
            "Epoch: 65 | Time: 1m 34s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 5.614 |  Val. PPL: 274.166\n",
            "Epoch: 66 | Time: 1m 34s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
            "\t Val. Loss: 5.653 |  Val. PPL: 285.123\n",
            "Epoch: 67 | Time: 1m 35s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
            "\t Val. Loss: 5.676 |  Val. PPL: 291.903\n",
            "Epoch: 68 | Time: 1m 35s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
            "\t Val. Loss: 5.755 |  Val. PPL: 315.721\n",
            "Epoch: 69 | Time: 1m 35s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
            "\t Val. Loss: 5.733 |  Val. PPL: 308.950\n",
            "Epoch: 70 | Time: 1m 35s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
            "\t Val. Loss: 5.703 |  Val. PPL: 299.877\n",
            "Epoch: 71 | Time: 1m 35s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 5.838 |  Val. PPL: 343.015\n",
            "Epoch: 72 | Time: 1m 35s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
            "\t Val. Loss: 5.808 |  Val. PPL: 332.972\n",
            "Epoch: 73 | Time: 1m 35s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
            "\t Val. Loss: 5.801 |  Val. PPL: 330.642\n",
            "Epoch: 74 | Time: 1m 35s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 5.825 |  Val. PPL: 338.769\n",
            "Epoch: 75 | Time: 1m 35s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 5.810 |  Val. PPL: 333.643\n",
            "Epoch: 76 | Time: 1m 35s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 5.831 |  Val. PPL: 340.586\n",
            "Epoch: 77 | Time: 1m 35s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
            "\t Val. Loss: 5.801 |  Val. PPL: 330.510\n",
            "Epoch: 78 | Time: 1m 35s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
            "\t Val. Loss: 5.861 |  Val. PPL: 351.171\n",
            "Epoch: 79 | Time: 1m 35s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
            "\t Val. Loss: 5.862 |  Val. PPL: 351.389\n",
            "Epoch: 80 | Time: 1m 35s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 5.900 |  Val. PPL: 365.116\n",
            "Epoch: 81 | Time: 1m 35s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 5.962 |  Val. PPL: 388.455\n",
            "Epoch: 82 | Time: 1m 34s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 5.908 |  Val. PPL: 368.012\n",
            "Epoch: 83 | Time: 1m 35s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 5.935 |  Val. PPL: 378.095\n",
            "Epoch: 84 | Time: 1m 34s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 5.913 |  Val. PPL: 369.891\n",
            "Epoch: 85 | Time: 1m 35s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 5.919 |  Val. PPL: 371.936\n",
            "Epoch: 86 | Time: 1m 35s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 5.921 |  Val. PPL: 372.639\n",
            "Epoch: 87 | Time: 1m 35s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 5.964 |  Val. PPL: 389.284\n",
            "Epoch: 88 | Time: 1m 35s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 5.981 |  Val. PPL: 395.783\n",
            "Epoch: 89 | Time: 1m 35s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 6.019 |  Val. PPL: 411.079\n",
            "Epoch: 90 | Time: 1m 35s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 6.080 |  Val. PPL: 436.959\n",
            "Epoch: 91 | Time: 1m 35s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 6.087 |  Val. PPL: 439.938\n",
            "Epoch: 92 | Time: 1m 35s\n",
            "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
            "\t Val. Loss: 6.061 |  Val. PPL: 428.732\n",
            "Epoch: 93 | Time: 1m 35s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 6.074 |  Val. PPL: 434.463\n",
            "Epoch: 94 | Time: 1m 34s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 6.096 |  Val. PPL: 443.870\n",
            "Epoch: 95 | Time: 1m 35s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
            "\t Val. Loss: 6.044 |  Val. PPL: 421.554\n",
            "Epoch: 96 | Time: 1m 34s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 6.093 |  Val. PPL: 442.528\n",
            "Epoch: 97 | Time: 1m 35s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
            "\t Val. Loss: 6.073 |  Val. PPL: 434.029\n",
            "Epoch: 98 | Time: 1m 34s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 6.067 |  Val. PPL: 431.339\n",
            "Epoch: 99 | Time: 1m 35s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 6.098 |  Val. PPL: 445.048\n",
            "Epoch: 100 | Time: 1m 35s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 6.065 |  Val. PPL: 430.501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOzG8D9PH77f"
      },
      "source": [
        "Creating a new model from the base model, by taking its decdoder and freezing its layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CefgDLaUGK2m",
        "outputId": "ad2042fe-d2d2-491e-a376-4bcb63c074c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, child in seq2seq.named_children():\n",
        "    print(name)\n",
        "\n",
        "trained_encoder = list(seq2seq.children())[0]\n",
        "trained_decoder = list(seq2seq.children())[1] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder\n",
            "decoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hZzSgZrGpd1"
      },
      "source": [
        "# Freezing the decoder layers\n",
        "for param in trained_decoder.parameters():    \n",
        "    param.requires_grad = False\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = trained_decoder\n",
        "\n",
        "new_model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ1Y--A_IZo9"
      },
      "source": [
        "Training new model with the frozen decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmldDRuAIX3q",
        "outputId": "5d59b84a-3a05-46d7-9dfa-63cae9f4b3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f'The model has {count_parameters(new_model):,} trainable parameters')\n",
        "\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(new_model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(new_model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(new_model.state_dict(), 'new-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,901,120 trainable parameters\n",
            "Epoch: 01 | Time: 1m 12s\n",
            "\tTrain Loss: 9.562 | Train PPL: 14217.408\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 02 | Time: 1m 11s\n",
            "\tTrain Loss: 9.542 | Train PPL: 13933.589\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 03 | Time: 1m 11s\n",
            "\tTrain Loss: 9.621 | Train PPL: 15077.078\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 04 | Time: 1m 11s\n",
            "\tTrain Loss: 9.617 | Train PPL: 15022.144\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 05 | Time: 1m 12s\n",
            "\tTrain Loss: 9.614 | Train PPL: 14967.571\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 06 | Time: 1m 12s\n",
            "\tTrain Loss: 9.571 | Train PPL: 14344.367\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 07 | Time: 1m 12s\n",
            "\tTrain Loss: 9.558 | Train PPL: 14156.732\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 08 | Time: 1m 12s\n",
            "\tTrain Loss: 9.553 | Train PPL: 14084.440\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 09 | Time: 1m 12s\n",
            "\tTrain Loss: 9.567 | Train PPL: 14281.219\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 10 | Time: 1m 12s\n",
            "\tTrain Loss: 9.549 | Train PPL: 14024.582\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 11 | Time: 1m 12s\n",
            "\tTrain Loss: 9.572 | Train PPL: 14358.436\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 12 | Time: 1m 12s\n",
            "\tTrain Loss: 9.575 | Train PPL: 14405.566\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 13 | Time: 1m 12s\n",
            "\tTrain Loss: 9.556 | Train PPL: 14128.251\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 14 | Time: 1m 12s\n",
            "\tTrain Loss: 9.553 | Train PPL: 14079.919\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 15 | Time: 1m 12s\n",
            "\tTrain Loss: 9.537 | Train PPL: 13869.212\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 16 | Time: 1m 12s\n",
            "\tTrain Loss: 9.575 | Train PPL: 14393.249\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 17 | Time: 1m 12s\n",
            "\tTrain Loss: 9.531 | Train PPL: 13778.233\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 18 | Time: 1m 12s\n",
            "\tTrain Loss: 9.557 | Train PPL: 14137.269\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 19 | Time: 1m 11s\n",
            "\tTrain Loss: 9.583 | Train PPL: 14509.599\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 20 | Time: 1m 12s\n",
            "\tTrain Loss: 9.552 | Train PPL: 14076.027\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 21 | Time: 1m 12s\n",
            "\tTrain Loss: 9.629 | Train PPL: 15205.303\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 22 | Time: 1m 12s\n",
            "\tTrain Loss: 9.637 | Train PPL: 15319.664\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 23 | Time: 1m 12s\n",
            "\tTrain Loss: 9.496 | Train PPL: 13309.094\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 24 | Time: 1m 11s\n",
            "\tTrain Loss: 9.614 | Train PPL: 14969.353\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 25 | Time: 1m 12s\n",
            "\tTrain Loss: 9.606 | Train PPL: 14846.370\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 26 | Time: 1m 12s\n",
            "\tTrain Loss: 9.585 | Train PPL: 14538.965\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 27 | Time: 1m 11s\n",
            "\tTrain Loss: 9.578 | Train PPL: 14449.086\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 28 | Time: 1m 12s\n",
            "\tTrain Loss: 9.576 | Train PPL: 14416.434\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 29 | Time: 1m 11s\n",
            "\tTrain Loss: 9.558 | Train PPL: 14161.896\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 30 | Time: 1m 12s\n",
            "\tTrain Loss: 9.531 | Train PPL: 13784.665\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 31 | Time: 1m 12s\n",
            "\tTrain Loss: 9.548 | Train PPL: 14016.077\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 32 | Time: 1m 12s\n",
            "\tTrain Loss: 9.544 | Train PPL: 13959.650\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 33 | Time: 1m 12s\n",
            "\tTrain Loss: 9.581 | Train PPL: 14488.688\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 34 | Time: 1m 12s\n",
            "\tTrain Loss: 9.534 | Train PPL: 13816.988\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 35 | Time: 1m 12s\n",
            "\tTrain Loss: 9.542 | Train PPL: 13932.150\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 36 | Time: 1m 11s\n",
            "\tTrain Loss: 9.544 | Train PPL: 13961.582\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 37 | Time: 1m 11s\n",
            "\tTrain Loss: 9.526 | Train PPL: 13711.500\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 38 | Time: 1m 11s\n",
            "\tTrain Loss: 9.575 | Train PPL: 14405.520\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 39 | Time: 1m 12s\n",
            "\tTrain Loss: 9.557 | Train PPL: 14150.002\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 40 | Time: 1m 12s\n",
            "\tTrain Loss: 9.521 | Train PPL: 13645.021\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 41 | Time: 1m 11s\n",
            "\tTrain Loss: 9.520 | Train PPL: 13623.070\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 42 | Time: 1m 11s\n",
            "\tTrain Loss: 9.583 | Train PPL: 14519.434\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 43 | Time: 1m 11s\n",
            "\tTrain Loss: 9.574 | Train PPL: 14392.260\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 44 | Time: 1m 11s\n",
            "\tTrain Loss: 9.590 | Train PPL: 14619.274\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 45 | Time: 1m 11s\n",
            "\tTrain Loss: 9.602 | Train PPL: 14793.881\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 46 | Time: 1m 12s\n",
            "\tTrain Loss: 9.632 | Train PPL: 15247.195\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 47 | Time: 1m 12s\n",
            "\tTrain Loss: 9.571 | Train PPL: 14340.907\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 48 | Time: 1m 11s\n",
            "\tTrain Loss: 9.534 | Train PPL: 13824.901\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 49 | Time: 1m 11s\n",
            "\tTrain Loss: 9.640 | Train PPL: 15371.422\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n",
            "Epoch: 50 | Time: 1m 12s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14686.374\n",
            "\t Val. Loss: 11.192 |  Val. PPL: 72522.477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9vz3ZJOIbad"
      },
      "source": [
        "Unfreezing the decoder and training some more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YkgIuTiIjAJ",
        "outputId": "d385b8c4-a94b-4770-a176-0bab3c2b00e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for param in new_model.parameters():    \n",
        "    param.requires_grad = True\n",
        "\n",
        "print(f'The model has {count_parameters(new_model):,} trainable parameters')\n",
        "\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(new_model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(new_model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(new_model.state_dict(), 'new-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 20,518,917 trainable parameters\n",
            "Epoch: 01 | Time: 1m 35s\n",
            "\tTrain Loss: 3.937 | Train PPL:  51.274\n",
            "\t Val. Loss: 3.898 |  Val. PPL:  49.303\n",
            "Epoch: 02 | Time: 1m 36s\n",
            "\tTrain Loss: 3.024 | Train PPL:  20.584\n",
            "\t Val. Loss: 3.650 |  Val. PPL:  38.490\n",
            "Epoch: 03 | Time: 1m 35s\n",
            "\tTrain Loss: 2.770 | Train PPL:  15.966\n",
            "\t Val. Loss: 3.599 |  Val. PPL:  36.564\n",
            "Epoch: 04 | Time: 1m 35s\n",
            "\tTrain Loss: 2.623 | Train PPL:  13.776\n",
            "\t Val. Loss: 3.533 |  Val. PPL:  34.239\n",
            "Epoch: 05 | Time: 1m 35s\n",
            "\tTrain Loss: 2.464 | Train PPL:  11.752\n",
            "\t Val. Loss: 3.551 |  Val. PPL:  34.863\n",
            "Epoch: 06 | Time: 1m 35s\n",
            "\tTrain Loss: 2.389 | Train PPL:  10.904\n",
            "\t Val. Loss: 3.528 |  Val. PPL:  34.062\n",
            "Epoch: 07 | Time: 1m 35s\n",
            "\tTrain Loss: 2.314 | Train PPL:  10.117\n",
            "\t Val. Loss: 3.496 |  Val. PPL:  32.970\n",
            "Epoch: 08 | Time: 1m 35s\n",
            "\tTrain Loss: 2.216 | Train PPL:   9.172\n",
            "\t Val. Loss: 3.537 |  Val. PPL:  34.373\n",
            "Epoch: 09 | Time: 1m 36s\n",
            "\tTrain Loss: 2.192 | Train PPL:   8.957\n",
            "\t Val. Loss: 3.520 |  Val. PPL:  33.789\n",
            "Epoch: 10 | Time: 1m 35s\n",
            "\tTrain Loss: 2.112 | Train PPL:   8.262\n",
            "\t Val. Loss: 3.480 |  Val. PPL:  32.453\n",
            "Epoch: 11 | Time: 1m 35s\n",
            "\tTrain Loss: 2.064 | Train PPL:   7.878\n",
            "\t Val. Loss: 3.530 |  Val. PPL:  34.141\n",
            "Epoch: 12 | Time: 1m 35s\n",
            "\tTrain Loss: 2.022 | Train PPL:   7.554\n",
            "\t Val. Loss: 3.504 |  Val. PPL:  33.233\n",
            "Epoch: 13 | Time: 1m 35s\n",
            "\tTrain Loss: 2.025 | Train PPL:   7.577\n",
            "\t Val. Loss: 3.484 |  Val. PPL:  32.578\n",
            "Epoch: 14 | Time: 1m 35s\n",
            "\tTrain Loss: 1.968 | Train PPL:   7.159\n",
            "\t Val. Loss: 3.494 |  Val. PPL:  32.914\n",
            "Epoch: 15 | Time: 1m 35s\n",
            "\tTrain Loss: 1.973 | Train PPL:   7.191\n",
            "\t Val. Loss: 3.505 |  Val. PPL:  33.270\n",
            "Epoch: 16 | Time: 1m 36s\n",
            "\tTrain Loss: 1.927 | Train PPL:   6.872\n",
            "\t Val. Loss: 3.489 |  Val. PPL:  32.749\n",
            "Epoch: 17 | Time: 1m 35s\n",
            "\tTrain Loss: 1.872 | Train PPL:   6.503\n",
            "\t Val. Loss: 3.559 |  Val. PPL:  35.127\n",
            "Epoch: 18 | Time: 1m 35s\n",
            "\tTrain Loss: 1.869 | Train PPL:   6.479\n",
            "\t Val. Loss: 3.529 |  Val. PPL:  34.090\n",
            "Epoch: 19 | Time: 1m 36s\n",
            "\tTrain Loss: 1.830 | Train PPL:   6.237\n",
            "\t Val. Loss: 3.548 |  Val. PPL:  34.735\n",
            "Epoch: 20 | Time: 1m 35s\n",
            "\tTrain Loss: 1.857 | Train PPL:   6.403\n",
            "\t Val. Loss: 3.530 |  Val. PPL:  34.110\n",
            "Epoch: 21 | Time: 1m 35s\n",
            "\tTrain Loss: 1.797 | Train PPL:   6.032\n",
            "\t Val. Loss: 3.524 |  Val. PPL:  33.908\n",
            "Epoch: 22 | Time: 1m 35s\n",
            "\tTrain Loss: 1.788 | Train PPL:   5.978\n",
            "\t Val. Loss: 3.554 |  Val. PPL:  34.957\n",
            "Epoch: 23 | Time: 1m 35s\n",
            "\tTrain Loss: 1.784 | Train PPL:   5.956\n",
            "\t Val. Loss: 3.545 |  Val. PPL:  34.635\n",
            "Epoch: 24 | Time: 1m 35s\n",
            "\tTrain Loss: 1.759 | Train PPL:   5.804\n",
            "\t Val. Loss: 3.510 |  Val. PPL:  33.459\n",
            "Epoch: 25 | Time: 1m 35s\n",
            "\tTrain Loss: 1.734 | Train PPL:   5.661\n",
            "\t Val. Loss: 3.548 |  Val. PPL:  34.752\n",
            "Epoch: 26 | Time: 1m 35s\n",
            "\tTrain Loss: 1.694 | Train PPL:   5.442\n",
            "\t Val. Loss: 3.523 |  Val. PPL:  33.903\n",
            "Epoch: 27 | Time: 1m 35s\n",
            "\tTrain Loss: 1.690 | Train PPL:   5.421\n",
            "\t Val. Loss: 3.583 |  Val. PPL:  35.983\n",
            "Epoch: 28 | Time: 1m 35s\n",
            "\tTrain Loss: 1.683 | Train PPL:   5.379\n",
            "\t Val. Loss: 3.594 |  Val. PPL:  36.366\n",
            "Epoch: 29 | Time: 1m 36s\n",
            "\tTrain Loss: 1.673 | Train PPL:   5.331\n",
            "\t Val. Loss: 3.574 |  Val. PPL:  35.675\n",
            "Epoch: 30 | Time: 1m 35s\n",
            "\tTrain Loss: 1.650 | Train PPL:   5.208\n",
            "\t Val. Loss: 3.600 |  Val. PPL:  36.588\n",
            "Epoch: 31 | Time: 1m 35s\n",
            "\tTrain Loss: 1.654 | Train PPL:   5.227\n",
            "\t Val. Loss: 3.579 |  Val. PPL:  35.824\n",
            "Epoch: 32 | Time: 1m 35s\n",
            "\tTrain Loss: 1.620 | Train PPL:   5.053\n",
            "\t Val. Loss: 3.640 |  Val. PPL:  38.088\n",
            "Epoch: 33 | Time: 1m 35s\n",
            "\tTrain Loss: 1.620 | Train PPL:   5.051\n",
            "\t Val. Loss: 3.590 |  Val. PPL:  36.228\n",
            "Epoch: 34 | Time: 1m 35s\n",
            "\tTrain Loss: 1.576 | Train PPL:   4.833\n",
            "\t Val. Loss: 3.615 |  Val. PPL:  37.169\n",
            "Epoch: 35 | Time: 1m 35s\n",
            "\tTrain Loss: 1.601 | Train PPL:   4.956\n",
            "\t Val. Loss: 3.595 |  Val. PPL:  36.422\n",
            "Epoch: 36 | Time: 1m 35s\n",
            "\tTrain Loss: 1.564 | Train PPL:   4.778\n",
            "\t Val. Loss: 3.656 |  Val. PPL:  38.691\n",
            "Epoch: 37 | Time: 1m 35s\n",
            "\tTrain Loss: 1.566 | Train PPL:   4.786\n",
            "\t Val. Loss: 3.642 |  Val. PPL:  38.170\n",
            "Epoch: 38 | Time: 1m 35s\n",
            "\tTrain Loss: 1.550 | Train PPL:   4.709\n",
            "\t Val. Loss: 3.658 |  Val. PPL:  38.777\n",
            "Epoch: 39 | Time: 1m 35s\n",
            "\tTrain Loss: 1.525 | Train PPL:   4.596\n",
            "\t Val. Loss: 3.682 |  Val. PPL:  39.740\n",
            "Epoch: 40 | Time: 1m 35s\n",
            "\tTrain Loss: 1.542 | Train PPL:   4.673\n",
            "\t Val. Loss: 3.694 |  Val. PPL:  40.196\n",
            "Epoch: 41 | Time: 1m 35s\n",
            "\tTrain Loss: 1.525 | Train PPL:   4.595\n",
            "\t Val. Loss: 3.660 |  Val. PPL:  38.881\n",
            "Epoch: 42 | Time: 1m 35s\n",
            "\tTrain Loss: 1.522 | Train PPL:   4.582\n",
            "\t Val. Loss: 3.670 |  Val. PPL:  39.242\n",
            "Epoch: 43 | Time: 1m 35s\n",
            "\tTrain Loss: 1.503 | Train PPL:   4.493\n",
            "\t Val. Loss: 3.680 |  Val. PPL:  39.650\n",
            "Epoch: 44 | Time: 1m 35s\n",
            "\tTrain Loss: 1.539 | Train PPL:   4.659\n",
            "\t Val. Loss: 3.663 |  Val. PPL:  38.981\n",
            "Epoch: 45 | Time: 1m 35s\n",
            "\tTrain Loss: 1.474 | Train PPL:   4.369\n",
            "\t Val. Loss: 3.674 |  Val. PPL:  39.416\n",
            "Epoch: 46 | Time: 1m 35s\n",
            "\tTrain Loss: 1.481 | Train PPL:   4.398\n",
            "\t Val. Loss: 3.669 |  Val. PPL:  39.206\n",
            "Epoch: 47 | Time: 1m 35s\n",
            "\tTrain Loss: 1.458 | Train PPL:   4.298\n",
            "\t Val. Loss: 3.699 |  Val. PPL:  40.400\n",
            "Epoch: 48 | Time: 1m 35s\n",
            "\tTrain Loss: 1.464 | Train PPL:   4.324\n",
            "\t Val. Loss: 3.720 |  Val. PPL:  41.255\n",
            "Epoch: 49 | Time: 1m 35s\n",
            "\tTrain Loss: 1.458 | Train PPL:   4.296\n",
            "\t Val. Loss: 3.712 |  Val. PPL:  40.955\n",
            "Epoch: 50 | Time: 1m 35s\n",
            "\tTrain Loss: 1.441 | Train PPL:   4.224\n",
            "\t Val. Loss: 3.737 |  Val. PPL:  41.987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Md20XUFOWd"
      },
      "source": [
        "Defining methods to translate sentences and plot attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVsVSzGlFGWT"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]\n",
        "\n",
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45)\n",
        "    ax.set_yticklabels(['']+translation)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OOZjVGvFiC0"
      },
      "source": [
        "Testing the model for translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUs9FGROFgPB",
        "outputId": "b8911fc7-8c98-4d97-9178-c1a614ee3a43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "example_idx = 12\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['src']\n",
        "trg = vars(train_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['ein', 'schwarzer', 'hund', 'und', 'ein', 'gefleckter', 'hund', 'kämpfen', '.']\n",
            "trg = ['a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsYoJh9dS01J"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHOqXuoeFmUP",
        "outputId": "6760ce08-cc2e-4640-a3e9-9936c5a88fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, seq2seq, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')\n",
        "display_attention(src, translation, attention)\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, new_model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['the', 'blue', 'with', 'blue', 'blue', 'open', 'blue', 'each', 'each', 'each', 'each', 'other', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'fighting', '.', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DaQhR8WZrEq"
      },
      "source": [
        "Calculating the bleu scores and comparing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoueDITfKtSJ",
        "outputId": "4f9471cf-40b0-4a9e-fa22-1575d0c06df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Calculating score for base model\n",
        "\n",
        "avg_score = 0\n",
        "j = 0\n",
        "\n",
        "for x in test_data:\n",
        "  src = vars(test_data.examples[j])['src']\n",
        "  trg = vars(test_data.examples[j])['trg']\n",
        "  translation, attention = translate_sentence(src, SRC, TRG, seq2seq, device)\n",
        "  score = sentence_bleu(translation, trg, weights=(1, 0, 0, 0))\n",
        "  avg_score = avg_score + score\n",
        "  j = j + 1\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------------\")\n",
        "avg_score = avg_score/len(test_data)\n",
        "print(f'BLEU score for base seq2seq model = {avg_score*100:.2f}')\n",
        "\n",
        "\n",
        "# Calculating score for new model\n",
        "\n",
        "avg_score = 0\n",
        "j = 0\n",
        "\n",
        "for x in test_data:\n",
        "  src = vars(test_data.examples[j])['src']\n",
        "  trg = vars(test_data.examples[j])['trg']\n",
        "  translation, attention = translate_sentence(src, SRC, TRG, new_model, device)\n",
        "  score = sentence_bleu(translation, trg, weights=(1, 0, 0, 0))\n",
        "  avg_score = avg_score + score\n",
        "  j = j + 1\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------------\")\n",
        "avg_score = avg_score/len(test_data)\n",
        "print(f'BLEU score for finetuned seq2seq model = {avg_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------------\n",
            "BLEU score for base seq2seq model = 13.53\n",
            "-----------------------------------------------------------------------------------------------\n",
            "BLEU score for finetuned seq2seq model = 15.46\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}