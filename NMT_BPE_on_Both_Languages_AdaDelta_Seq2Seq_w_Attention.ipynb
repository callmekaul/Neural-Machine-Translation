{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT - BPE on Both Languages - AdaDelta - Seq2Seq w Attention",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI3efIKp0m2G"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb4Yoa7IFn6k"
      },
      "source": [
        "Dataset taken from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ussehQFZ0sOV"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
        " \n",
        "file_path = os.path.dirname(zip_path)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v-g7pDRFtd3"
      },
      "source": [
        "Pre Processsing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKjUY5gj0zqE"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "  \n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        " \n",
        "  w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)             #add space before and after special charachters\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)                    #replace more than 1 consecutive space with a single space\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)           #replace all charachters except these with space\n",
        " \n",
        "  w = w.strip()\n",
        " \n",
        "  #w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        " \n",
        "def preprocess_sentence2(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        " \n",
        "  w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)             #add space before and after special charachters\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)                    #replace more than 1 consecutive space with a single space\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)           #replace all charachters except these with space\n",
        " \n",
        "  w = w.strip()\n",
        " \n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKSVvCiFPKV"
      },
      "source": [
        "Change number of senetences in the following block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th6NtJfy58as",
        "outputId": "b0cea846-2374-43c8-bbeb-1920dca5e7e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        " \n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  print(word_pairs[25])\n",
        "  return zip(*word_pairs)\n",
        " \n",
        "en, sp = create_dataset(file_path, None)          #change number of sentences here\n",
        " \n",
        "en2 = en\n",
        "sp2 = sp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i ran .', 'corria .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql9stK0lFWN4"
      },
      "source": [
        "Implementation to Learn the Byte Pair Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgujfBFR1MkL"
      },
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re, copy\n",
        "\n",
        "def get_vocabulary(fobj):\n",
        "    \"\"\"Read text and return dictionary that encodes vocabulary\n",
        "    \"\"\"\n",
        "    vocab_en = Counter()\n",
        "    for line in fobj:\n",
        "        for word in line.split():\n",
        "            vocab_en[word] += 1\n",
        "    return vocab_en\n",
        "\n",
        "def update_pair_statistics(pair, changed, stats, indices):\n",
        "    \"\"\"Minimally update the indices_en and frequency of symbol pairs\n",
        "\n",
        "    if we merge a pair of symbols_en, only pairs that overlap with occurrences\n",
        "    of this pair are affected, and need to be updated.\n",
        "    \"\"\"\n",
        "    stats[pair] = 0\n",
        "    indices[pair] = defaultdict(int)\n",
        "    first, second = pair\n",
        "    new_pair = first+second\n",
        "    for j, word, old_word, freq in changed:\n",
        "\n",
        "        # find all instances of pair, and update frequency/indices_en around it\n",
        "        i = 0\n",
        "        while True:\n",
        "            try:\n",
        "                i = old_word.index(first, i)\n",
        "            except ValueError:\n",
        "                break\n",
        "            if i < len(old_word)-1 and old_word[i+1] == second:\n",
        "                if i:\n",
        "                    prev = old_word[i-1:i+1]\n",
        "                    stats[prev] -= freq\n",
        "                    indices[prev][j] -= 1\n",
        "                if i < len(old_word)-2:\n",
        "                    # don't double-count consecutive pairs\n",
        "                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n",
        "                        nex = old_word[i+1:i+3]\n",
        "                        stats[nex] -= freq\n",
        "                        indices[nex][j] -= 1\n",
        "                i += 2\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        i = 0\n",
        "        while True:\n",
        "            try:\n",
        "                i = word.index(new_pair, i)\n",
        "            except ValueError:\n",
        "                break\n",
        "            if i:\n",
        "                prev = word[i-1:i+1]\n",
        "                stats[prev] += freq\n",
        "                indices[prev][j] += 1\n",
        "            # don't double-count consecutive pairs\n",
        "            if i < len(word)-1 and word[i+1] != new_pair:\n",
        "                nex = word[i:i+2]\n",
        "                stats[nex] += freq\n",
        "                indices[nex][j] += 1\n",
        "            i += 1\n",
        "\n",
        "\n",
        "def get_pair_statistics(vocab):\n",
        "    \"\"\"Count frequency of all symbol pairs, and create index\"\"\"\n",
        "\n",
        "    # data structure of pair frequencies\n",
        "    stats = defaultdict(int)\n",
        "\n",
        "    #index from pairs to words\n",
        "    indices = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for i, (word, freq) in enumerate(vocab):\n",
        "        prev_char = word[0]\n",
        "        for char in word[1:]:\n",
        "            stats[prev_char, char] += freq\n",
        "            indices[prev_char, char][i] += 1\n",
        "            prev_char = char\n",
        "\n",
        "    return stats, indices\n",
        "\n",
        "\n",
        "def replace_pair(pair, vocab, indices):\n",
        "    \"\"\"Replace all occurrences of a symbol pair ('A', 'B') with a new symbol 'AB'\"\"\"\n",
        "    first, second = pair\n",
        "    pair_str = ''.join(pair)\n",
        "    pair_str = pair_str.replace('\\\\','\\\\\\\\')\n",
        "    changes = []\n",
        "    pattern = re.compile(r'(?<!\\S)' + re.escape(first + ' ' + second) + r'(?!\\S)')\n",
        "    # if sys.version_info < (3, 0):\n",
        "    #     iterator = indices[pair].iteritems()\n",
        "    # else:\n",
        "    iterator = indices[pair].items()\n",
        "    for j, freq in iterator:\n",
        "        if freq < 1:\n",
        "            continue\n",
        "        word, freq = vocab[j]\n",
        "        new_word = ' '.join(word)\n",
        "        new_word = pattern.sub(pair_str, new_word)\n",
        "        new_word = tuple(new_word.split())\n",
        "\n",
        "        vocab[j] = (new_word, freq)\n",
        "        changes.append((j, new_word, word, freq))\n",
        "\n",
        "    return changes\n",
        "\n",
        "def prune_stats(stats, big_stats, threshold):\n",
        "    \"\"\"Prune statistics dict for efficiency of max()\n",
        "\n",
        "    The frequency of a symbol pair never increases, so pruning is generally safe\n",
        "    (until we the most frequent pair is less frequent than a pair we previously pruned)\n",
        "    big_stats_en keeps full statistics for when we need to access pruned items\n",
        "    \"\"\"\n",
        "    for item,freq in list(stats.items()):\n",
        "        if freq < threshold:\n",
        "            del stats[item]\n",
        "            if freq < 0:\n",
        "                big_stats_en[item] += freq\n",
        "            else:\n",
        "                big_stats_en[item] = freq\n",
        "\n",
        "\n",
        "#ENGLISH\n",
        "\n",
        "\n",
        "vocab_en = get_vocabulary(en)\n",
        "vocab_en = dict([(tuple(x)+('</w>',) ,y) for (x,y) in vocab_en.items()])\n",
        "sorted_vocab_en = sorted(vocab_en.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "stats_en, indices_en = get_pair_statistics(sorted_vocab_en)\n",
        "big_stats_en = copy.deepcopy(stats_en)\n",
        "# threshold is inspired by Zipfian assumption, but should only affect speed\n",
        "threshold_en = max(stats_en.values()) / 10\n",
        "\n",
        "outF_en = open(\"codes_en.txt\" , \"w\")\n",
        "symbols_en = 3000\n",
        "\n",
        "for i in range(symbols_en):\n",
        "    if stats_en:\n",
        "        most_frequent_en = max(stats_en, key=stats_en.get)\n",
        "\n",
        "    # we probably missed the best pair because of pruning; go back to full statistics\n",
        "    if not stats_en or (i and stats_en[most_frequent_en] < threshold_en):\n",
        "        prune_stats(stats_en, big_stats_en, threshold_en)\n",
        "        stats_en = copy.deepcopy(big_stats_en)\n",
        "        most_frequent_en = max(stats_en, key=stats_en.get)\n",
        "        # threshold_en is inspired by Zipfian assumption, but should only affect speed\n",
        "        threshold_en = stats_en[most_frequent_en] * i/(i+10000.0)\n",
        "        prune_stats(stats_en, big_stats_en, threshold_en)\n",
        "    \n",
        "    if stats_en[most_frequent_en] < 2:\n",
        "        print('no pair has frequency > 1. Stopping\\n')\n",
        "        break\n",
        "    \n",
        "    outF_en.write('{0} {1}\\n'.format(*most_frequent_en))\n",
        "    changes_en = replace_pair(most_frequent_en, sorted_vocab_en, indices_en)\n",
        "    update_pair_statistics(most_frequent_en, changes_en, stats_en, indices_en)\n",
        "    stats_en[most_frequent_en] = 0\n",
        "    if not i % 100:\n",
        "        prune_stats(stats_en, big_stats_en, threshold_en)\n",
        "\n",
        "\n",
        "#SPANISH\n",
        "\n",
        "\n",
        "vocab_sp = get_vocabulary(sp)\n",
        "vocab_sp = dict([(tuple(x)+('</w>',) ,y) for (x,y) in vocab_sp.items()])\n",
        "sorted_vocab_sp = sorted(vocab_sp.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "stats_sp, indices_sp = get_pair_statistics(sorted_vocab_sp)\n",
        "big_stats_sp = copy.deepcopy(stats_sp)\n",
        "# threshold is inspired by Zipfian assumption, but should only affect speed\n",
        "threshold_sp = max(stats_sp.values()) / 10\n",
        "\n",
        "outF_sp = open(\"codes_sp.txt\" , \"w\")\n",
        "symbols_sp = 3000\n",
        "\n",
        "for i in range(symbols_sp):\n",
        "    if stats_sp:\n",
        "        most_frequent_sp = max(stats_sp, key=stats_sp.get)\n",
        "\n",
        "    # we probably missed the best pair because of pruning; go back to full statistics\n",
        "    if not stats_sp or (i and stats_sp[most_frequent_sp] < threshold_sp):\n",
        "        prune_stats(stats_sp, big_stats_sp, threshold_sp)\n",
        "        stats_sp = copy.deepcopy(big_stats_sp)\n",
        "        most_frequent_sp = max(stats_sp, key=stats_sp.get)\n",
        "        # threshold_sp is inspired by Zipfian assumption, but should only affect speed\n",
        "        threshold_sp = stats_sp[most_frequent_sp] * i/(i+10000.0)\n",
        "        prune_stats(stats_sp, big_stats_sp, threshold_sp)\n",
        "    \n",
        "    if stats_sp[most_frequent_sp] < 2:\n",
        "        print('no pair has frequency > 1. Stopping\\n')\n",
        "        break\n",
        "    \n",
        "    outF_sp.write('{0} {1}\\n'.format(*most_frequent_sp))\n",
        "    changes_sp = replace_pair(most_frequent_sp, sorted_vocab_sp, indices_sp)\n",
        "    update_pair_statistics(most_frequent_sp, changes_sp, stats_sp, indices_sp)\n",
        "    stats_sp[most_frequent_sp] = 0\n",
        "    if not i % 100:\n",
        "        prune_stats(stats_sp, big_stats_sp, threshold_sp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLTof3LPFcHF"
      },
      "source": [
        "Implementation of the Encoder System using the Learned Codes from BPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE1lZK-61aGD"
      },
      "source": [
        "class BPE(object):\n",
        "\n",
        "    def __init__(self, codes, separator='@@'):\n",
        "        self.bpe_codes = [tuple(item.split()) for item in codes]\n",
        "        # some hacking to deal with duplicates (only consider first instance)\n",
        "        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n",
        "\n",
        "        self.separator = separator\n",
        "\n",
        "    def segment(self, sentence):\n",
        "        \"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"\n",
        "\n",
        "        output = []\n",
        "        for word in sentence.split():\n",
        "            new_word = encode(word, self.bpe_codes)\n",
        "\n",
        "            for item in new_word[:-1]:\n",
        "                output.append(item + self.separator)\n",
        "            output.append(new_word[-1])\n",
        "\n",
        "        return ' '.join(output)\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "\n",
        "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "def encode(orig, bpe_codes, cache={}):\n",
        "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n",
        "    \"\"\"\n",
        "\n",
        "    if orig in cache:\n",
        "        return cache[orig]\n",
        "\n",
        "    word = tuple(orig) + ('</w>',)\n",
        "    pairs = get_pairs(word)\n",
        "\n",
        "    while True:\n",
        "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
        "        if bigram not in bpe_codes:\n",
        "            break\n",
        "        first, second = bigram\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            try:\n",
        "                j = word.index(first, i)\n",
        "                new_word.extend(word[i:j])\n",
        "                i = j\n",
        "            except:\n",
        "                new_word.extend(word[i:])\n",
        "                break\n",
        "\n",
        "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                new_word.append(first+second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_word = tuple(new_word)\n",
        "        word = new_word\n",
        "        if len(word) == 1:\n",
        "            break\n",
        "        else:\n",
        "            pairs = get_pairs(word)\n",
        "\n",
        "    # don't print end-of-word symbols\n",
        "    if word[-1] == '</w>':\n",
        "        word = word[:-1]\n",
        "    elif word[-1].endswith('</w>'):\n",
        "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
        "\n",
        "    cache[orig] = word\n",
        "    return word\n",
        "\n",
        "\n",
        "#ENGLISH\n",
        "\n",
        "inF_en = open(\"codes_en.txt\" , \"r\")\n",
        "\n",
        "outF2_en = open(\"output_en.txt\" , \"w\")\n",
        "seperator_en = '|'\n",
        "bpe_en = BPE(inF_en, seperator_en)\n",
        "\n",
        "for line in en:\n",
        "  outF2_en.write(bpe_en.segment(line).strip())\n",
        "  outF2_en.write('\\n')\n",
        "\n",
        "#SPANISH\n",
        "\n",
        "inF_sp = open(\"codes_sp.txt\" , \"r\")\n",
        "\n",
        "outF2_sp = open(\"output_sp.txt\" , \"w\")\n",
        "seperator_sp = '|'\n",
        "bpe_sp = BPE(inF_sp, seperator_sp)\n",
        "\n",
        "for line in sp:\n",
        "  outF2_sp.write(bpe_sp.segment(line).strip())\n",
        "  outF2_sp.write('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvHNCcG8uhaC",
        "outputId": "bf0b9735-8fed-4fab-885d-eb65b762d97d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "C = 0\n",
        "\n",
        "inF2_en = open(\"output_en.txt\", \"r\")\n",
        "for line in inF2_en:\n",
        "  print(line)\n",
        "  C = C+1\n",
        "  if (C>=500):\n",
        "    break\n",
        "\n",
        "inF2_sp = open(\"output_sp.txt\", \"r\")\n",
        "for line in inF2_sp:\n",
        "  print(line)\n",
        "  C = C+1\n",
        "  if (C>=1000):\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "go .\n",
            "\n",
            "go .\n",
            "\n",
            "go .\n",
            "\n",
            "go .\n",
            "\n",
            "h| i .\n",
            "\n",
            "run !\n",
            "\n",
            "run .\n",
            "\n",
            "who ?\n",
            "\n",
            "fire !\n",
            "\n",
            "fire !\n",
            "\n",
            "fire !\n",
            "\n",
            "help !\n",
            "\n",
            "help !\n",
            "\n",
            "help !\n",
            "\n",
            "ju| mp !\n",
            "\n",
            "ju| mp .\n",
            "\n",
            "stop !\n",
            "\n",
            "stop !\n",
            "\n",
            "stop !\n",
            "\n",
            "wait !\n",
            "\n",
            "wait .\n",
            "\n",
            "go on .\n",
            "\n",
            "go on .\n",
            "\n",
            "hel| l| o !\n",
            "\n",
            "i ran .\n",
            "\n",
            "i ran .\n",
            "\n",
            "i try .\n",
            "\n",
            "i won !\n",
            "\n",
            "o| h no !\n",
            "\n",
            "rela| x .\n",
            "\n",
            "smile .\n",
            "\n",
            "attack !\n",
            "\n",
            "attack !\n",
            "\n",
            "get up .\n",
            "\n",
            "go now .\n",
            "\n",
            "got it !\n",
            "\n",
            "got it ?\n",
            "\n",
            "got it ?\n",
            "\n",
            "he ran .\n",
            "\n",
            "ho| p in .\n",
            "\n",
            "hu| g me .\n",
            "\n",
            "i fell .\n",
            "\n",
            "i know .\n",
            "\n",
            "i left .\n",
            "\n",
            "i lied .\n",
            "\n",
            "i lost .\n",
            "\n",
            "i quit .\n",
            "\n",
            "i quit .\n",
            "\n",
            "i work .\n",
            "\n",
            "i m .\n",
            "\n",
            "i m up .\n",
            "\n",
            "listen .\n",
            "\n",
            "listen .\n",
            "\n",
            "listen .\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "no way !\n",
            "\n",
            "really ?\n",
            "\n",
            "really ?\n",
            "\n",
            "thanks .\n",
            "\n",
            "thanks .\n",
            "\n",
            "try it .\n",
            "\n",
            "we try .\n",
            "\n",
            "we won .\n",
            "\n",
            "why me ?\n",
            "\n",
            "ask tom .\n",
            "\n",
            "a| we| some !\n",
            "\n",
            "be calm .\n",
            "\n",
            "be cool .\n",
            "\n",
            "be fair .\n",
            "\n",
            "be kind .\n",
            "\n",
            "be nice .\n",
            "\n",
            "beat it .\n",
            "\n",
            "call me .\n",
            "\n",
            "call me .\n",
            "\n",
            "call me .\n",
            "\n",
            "call us .\n",
            "\n",
            "come in .\n",
            "\n",
            "come in .\n",
            "\n",
            "come in .\n",
            "\n",
            "come on !\n",
            "\n",
            "come on .\n",
            "\n",
            "come on .\n",
            "\n",
            "drop it !\n",
            "\n",
            "get tom .\n",
            "\n",
            "get out !\n",
            "\n",
            "get out .\n",
            "\n",
            "get out .\n",
            "\n",
            "get out .\n",
            "\n",
            "get out .\n",
            "\n",
            "get out .\n",
            "\n",
            "go away !\n",
            "\n",
            "go away !\n",
            "\n",
            "go away !\n",
            "\n",
            "go away !\n",
            "\n",
            "go away !\n",
            "\n",
            "go away !\n",
            "\n",
            "go away !\n",
            "\n",
            "go away .\n",
            "\n",
            "go away .\n",
            "\n",
            "go away .\n",
            "\n",
            "go away .\n",
            "\n",
            "go away .\n",
            "\n",
            "go away .\n",
            "\n",
            "go away .\n",
            "\n",
            "go home .\n",
            "\n",
            "go s| low .\n",
            "\n",
            "good| b| ye !\n",
            "\n",
            "good| b| ye !\n",
            "\n",
            "good| b| ye !\n",
            "\n",
            "hang on !\n",
            "\n",
            "hang on !\n",
            "\n",
            "hang on !\n",
            "\n",
            "hang on .\n",
            "\n",
            "he came .\n",
            "\n",
            "he quit .\n",
            "\n",
            "help me !\n",
            "\n",
            "help me .\n",
            "\n",
            "help me .\n",
            "\n",
            "help me .\n",
            "\n",
            "help us .\n",
            "\n",
            "hit tom .\n",
            "\n",
            "hold it !\n",
            "\n",
            "hold on .\n",
            "\n",
            "hold on .\n",
            "\n",
            "hold on .\n",
            "\n",
            "hu| g tom .\n",
            "\n",
            "i agree .\n",
            "\n",
            "i agree .\n",
            "\n",
            "i bo| wed .\n",
            "\n",
            "i moved .\n",
            "\n",
            "i moved .\n",
            "\n",
            "i moved .\n",
            "\n",
            "i moved .\n",
            "\n",
            "i slept .\n",
            "\n",
            "i tried .\n",
            "\n",
            "i ll go .\n",
            "\n",
            "i m tom .\n",
            "\n",
            "i m fat .\n",
            "\n",
            "i m fat .\n",
            "\n",
            "i m fit .\n",
            "\n",
            "i m hit !\n",
            "\n",
            "i m old .\n",
            "\n",
            "i m shy .\n",
            "\n",
            "i m w| et .\n",
            "\n",
            "it s ok .\n",
            "\n",
            "it s me !\n",
            "\n",
            "it s me .\n",
            "\n",
            "join us .\n",
            "\n",
            "join us .\n",
            "\n",
            "keep it .\n",
            "\n",
            "me , too .\n",
            "\n",
            "open up .\n",
            "\n",
            "perfect !\n",
            "\n",
            "see you .\n",
            "\n",
            "show me .\n",
            "\n",
            "show me .\n",
            "\n",
            "show me .\n",
            "\n",
            "shut up !\n",
            "\n",
            "shut up !\n",
            "\n",
            "ski| p it .\n",
            "\n",
            "so long .\n",
            "\n",
            "so long .\n",
            "\n",
            "stop it .\n",
            "\n",
            "stop it .\n",
            "\n",
            "take it .\n",
            "\n",
            "tell me .\n",
            "\n",
            "tom ate .\n",
            "\n",
            "tom ran .\n",
            "\n",
            "tom won .\n",
            "\n",
            "wait up .\n",
            "\n",
            "wake up !\n",
            "\n",
            "wake up !\n",
            "\n",
            "wake up !\n",
            "\n",
            "wake up .\n",
            "\n",
            "wash up .\n",
            "\n",
            "we care .\n",
            "\n",
            "we know .\n",
            "\n",
            "we lost .\n",
            "\n",
            "welcome .\n",
            "\n",
            "welcome .\n",
            "\n",
            "who ate ?\n",
            "\n",
            "who ran ?\n",
            "\n",
            "who ran ?\n",
            "\n",
            "who won ?\n",
            "\n",
            "who won ?\n",
            "\n",
            "why not ?\n",
            "\n",
            "you run .\n",
            "\n",
            "you won .\n",
            "\n",
            "am i fat ?\n",
            "\n",
            "ask them .\n",
            "\n",
            "ask them .\n",
            "\n",
            "back off !\n",
            "\n",
            "back off .\n",
            "\n",
            "be a man .\n",
            "\n",
            "be bra| ve .\n",
            "\n",
            "be bri| ef .\n",
            "\n",
            "be bri| ef .\n",
            "\n",
            "be bri| ef .\n",
            "\n",
            "be quiet .\n",
            "\n",
            "be still .\n",
            "\n",
            "call tom .\n",
            "\n",
            "call tom .\n",
            "\n",
            "call tom .\n",
            "\n",
            "che| er up !\n",
            "\n",
            "che| er up .\n",
            "\n",
            "cool off !\n",
            "\n",
            "cu| ff him .\n",
            "\n",
            "don t go .\n",
            "\n",
            "drive on .\n",
            "\n",
            "find tom .\n",
            "\n",
            "find tom .\n",
            "\n",
            "find tom .\n",
            "\n",
            "find tom .\n",
            "\n",
            "find tom .\n",
            "\n",
            "find tom .\n",
            "\n",
            "fix this .\n",
            "\n",
            "get away !\n",
            "\n",
            "get away !\n",
            "\n",
            "get away !\n",
            "\n",
            "get away !\n",
            "\n",
            "get down !\n",
            "\n",
            "get down .\n",
            "\n",
            "get down .\n",
            "\n",
            "get lost !\n",
            "\n",
            "get lost !\n",
            "\n",
            "get lost !\n",
            "\n",
            "get lost !\n",
            "\n",
            "get lost !\n",
            "\n",
            "get lost .\n",
            "\n",
            "get real !\n",
            "\n",
            "get real !\n",
            "\n",
            "get real .\n",
            "\n",
            "go ahead !\n",
            "\n",
            "go ahead .\n",
            "\n",
            "go on in .\n",
            "\n",
            "go on in .\n",
            "\n",
            "go on in .\n",
            "\n",
            "go on in .\n",
            "\n",
            "go on in .\n",
            "\n",
            "good job !\n",
            "\n",
            "gra| b tom .\n",
            "\n",
            "gra| b him .\n",
            "\n",
            "have fun .\n",
            "\n",
            "have fun .\n",
            "\n",
            "have fun .\n",
            "\n",
            "he spoke .\n",
            "\n",
            "he tries .\n",
            "\n",
            "he tries .\n",
            "\n",
            "help tom .\n",
            "\n",
            "help him .\n",
            "\n",
            "h| i , guys .\n",
            "\n",
            "h| i , guys .\n",
            "\n",
            "h| i , guys .\n",
            "\n",
            "h| i , guys .\n",
            "\n",
            "h| i , guys .\n",
            "\n",
            "h| i , guys .\n",
            "\n",
            "how cu| te !\n",
            "\n",
            "how deep ?\n",
            "\n",
            "how deep ?\n",
            "\n",
            "hu| mor me .\n",
            "\n",
            "hu| mor me .\n",
            "\n",
            "hurry up .\n",
            "\n",
            "hurry up .\n",
            "\n",
            "i agreed .\n",
            "\n",
            "i agreed .\n",
            "\n",
            "i agreed .\n",
            "\n",
            "i am fat .\n",
            "\n",
            "i am old .\n",
            "\n",
            "i ate it .\n",
            "\n",
            "i ate it .\n",
            "\n",
            "i can go .\n",
            "\n",
            "i did ok .\n",
            "\n",
            "i did it .\n",
            "\n",
            "i failed .\n",
            "\n",
            "i forgot .\n",
            "\n",
            "i get by .\n",
            "\n",
            "i get it .\n",
            "\n",
            "i got it .\n",
            "\n",
            "i ph| oned .\n",
            "\n",
            "i re| fu| se .\n",
            "\n",
            "i re| sign .\n",
            "\n",
            "i re| sign .\n",
            "\n",
            "i saw it .\n",
            "\n",
            "i smiled .\n",
            "\n",
            "i stayed .\n",
            "\n",
            "i talked .\n",
            "\n",
            "i talked .\n",
            "\n",
            "i talked .\n",
            "\n",
            "i talked .\n",
            "\n",
            "i use it .\n",
            "\n",
            "i waited .\n",
            "\n",
            "i ll pay .\n",
            "\n",
            "i m back .\n",
            "\n",
            "i m back .\n",
            "\n",
            "i m ba| ld .\n",
            "\n",
            "i m ba| ld .\n",
            "\n",
            "i m calm .\n",
            "\n",
            "i m cool .\n",
            "\n",
            "i m done .\n",
            "\n",
            "i m easy .\n",
            "\n",
            "i m fair .\n",
            "\n",
            "i m fine .\n",
            "\n",
            "i m free !\n",
            "\n",
            "i m free .\n",
            "\n",
            "i m full .\n",
            "\n",
            "i m full .\n",
            "\n",
            "i m full .\n",
            "\n",
            "i m here .\n",
            "\n",
            "i m home .\n",
            "\n",
            "i m hurt .\n",
            "\n",
            "i m late .\n",
            "\n",
            "i m la| zy .\n",
            "\n",
            "i m lost .\n",
            "\n",
            "i m mean .\n",
            "\n",
            "i m next .\n",
            "\n",
            "i m o| k| ay .\n",
            "\n",
            "i m poor .\n",
            "\n",
            "i m rich .\n",
            "\n",
            "i m rich .\n",
            "\n",
            "i m safe .\n",
            "\n",
            "i m sick .\n",
            "\n",
            "i m th| in .\n",
            "\n",
            "i m ti| dy .\n",
            "\n",
            "i m warm .\n",
            "\n",
            "i m wea| k .\n",
            "\n",
            "i m wise .\n",
            "\n",
            "i ve won .\n",
            "\n",
            "it hel| ps .\n",
            "\n",
            "it hur| ts .\n",
            "\n",
            "it works .\n",
            "\n",
            "it s tom .\n",
            "\n",
            "it s fun .\n",
            "\n",
            "it s his .\n",
            "\n",
            "it s new .\n",
            "\n",
            "it s od| d .\n",
            "\n",
            "it s old .\n",
            "\n",
            "it s red .\n",
            "\n",
            "it s sad .\n",
            "\n",
            "keep out !\n",
            "\n",
            "keep out .\n",
            "\n",
            "kiss tom .\n",
            "\n",
            "kiss tom .\n",
            "\n",
            "kiss tom .\n",
            "\n",
            "leave it .\n",
            "\n",
            "leave me .\n",
            "\n",
            "leave us .\n",
            "\n",
            "let s go !\n",
            "\n",
            "let s go !\n",
            "\n",
            "look out !\n",
            "\n",
            "marry me .\n",
            "\n",
            "may i go ?\n",
            "\n",
            "save tom .\n",
            "\n",
            "she came .\n",
            "\n",
            "she died .\n",
            "\n",
            "she run| s .\n",
            "\n",
            "sit down !\n",
            "\n",
            "sit down .\n",
            "\n",
            "sit here .\n",
            "\n",
            "speak up !\n",
            "\n",
            "speak up !\n",
            "\n",
            "speak up !\n",
            "\n",
            "stand by .\n",
            "\n",
            "stand by .\n",
            "\n",
            "stand up !\n",
            "\n",
            "stand up !\n",
            "\n",
            "stay put .\n",
            "\n",
            "stop tom .\n",
            "\n",
            "take tom .\n",
            "\n",
            "tell tom .\n",
            "\n",
            "terri| fic !\n",
            "\n",
            "terri| fic !\n",
            "\n",
            "they won .\n",
            "\n",
            "tom came .\n",
            "\n",
            "tom died .\n",
            "\n",
            "tom fell .\n",
            "\n",
            "tom knew .\n",
            "\n",
            "tom knew .\n",
            "\n",
            "tom left .\n",
            "\n",
            "tom lied .\n",
            "\n",
            "tom lies .\n",
            "\n",
            "tom lost .\n",
            "\n",
            "tom paid .\n",
            "\n",
            "tom quit .\n",
            "\n",
            "tom s| wa| m .\n",
            "\n",
            "tom we| pt .\n",
            "\n",
            "tom s up .\n",
            "\n",
            "too late .\n",
            "\n",
            "trust me .\n",
            "\n",
            "try hard .\n",
            "\n",
            "try some .\n",
            "\n",
            "try some .\n",
            "\n",
            "try some .\n",
            "\n",
            "try this .\n",
            "\n",
            "try this .\n",
            "\n",
            "try this .\n",
            "\n",
            "use this .\n",
            "\n",
            "war| n tom .\n",
            "\n",
            "war| n tom .\n",
            "\n",
            "war| n tom .\n",
            "\n",
            "war| n tom .\n",
            "\n",
            "watch me .\n",
            "\n",
            "watch me .\n",
            "\n",
            "watch me .\n",
            "\n",
            "watch us .\n",
            "\n",
            "watch us .\n",
            "\n",
            "we agree .\n",
            "\n",
            "we tried .\n",
            "\n",
            "we ll go .\n",
            "\n",
            "we re ok .\n",
            "\n",
            "what for ?\n",
            "\n",
            "what fun !\n",
            "\n",
            "who am i ?\n",
            "\n",
            "who came ?\n",
            "\n",
            "who died ?\n",
            "\n",
            "who fell ?\n",
            "\n",
            "who fell ?\n",
            "\n",
            "who quit ?\n",
            "\n",
            "who quit ?\n",
            "\n",
            "who s| wa| m ?\n",
            "\n",
            "who s he ?\n",
            "\n",
            "write me .\n",
            "\n",
            "after you .\n",
            "\n",
            "after you .\n",
            "\n",
            "after you .\n",
            "\n",
            "after you .\n",
            "\n",
            "after you .\n",
            "\n",
            "ai| m . fire !\n",
            "\n",
            "answer me .\n",
            "\n",
            "answer me .\n",
            "\n",
            "answer me .\n",
            "\n",
            "birds fly .\n",
            "\n",
            "b| less you .\n",
            "\n",
            "call home !\n",
            "\n",
            "calm down .\n",
            "\n",
            "calm down .\n",
            "\n",
            "calm down .\n",
            "\n",
            "calm down .\n",
            "\n",
            "can we go ?\n",
            "\n",
            "can we go ?\n",
            "\n",
            "can we go ?\n",
            "\n",
            "catch tom .\n",
            "\n",
            "catch him .\n",
            "\n",
            "catch him .\n",
            "\n",
            "catch him .\n",
            "\n",
            "catch him .\n",
            "\n",
            "come back .\n",
            "\n",
            "come here .\n",
            "\n",
            "come here .\n",
            "\n",
            "come here .\n",
            "\n",
            "come home .\n",
            "\n",
            "come over .\n",
            "\n",
            "come over .\n",
            "\n",
            "come over .\n",
            "\n",
            "come over .\n",
            "\n",
            "come soon .\n",
            "\n",
            "do it now .\n",
            "\n",
            "dogs bar| k .\n",
            "\n",
            "don t ask .\n",
            "\n",
            "don t cry .\n",
            "\n",
            "don t cry .\n",
            "\n",
            "don t cry .\n",
            "\n",
            "don t cry .\n",
            "\n",
            "don t lie .\n",
            "\n",
            "don t run .\n",
            "\n",
            "f| anta| stic !\n",
            "\n",
            "f| anta| stic !\n",
            "\n",
            "f| anta| stic !\n",
            "\n",
            "feel this .\n",
            "\n",
            "feel this .\n",
            "\n",
            "feel this .\n",
            "\n",
            "feel this .\n",
            "\n",
            "feel this .\n",
            "\n",
            "follow me .\n",
            "\n",
            "follow me .\n",
            "\n",
            "follow us .\n",
            "\n",
            "forget it .\n",
            "\n",
            "forget me .\n",
            "\n",
            "forget me .\n",
            "\n",
            "forget me .\n",
            "\n",
            "forget me .\n",
            "\n",
            "get ready .\n",
            "\n",
            "go for it .\n",
            "\n",
            "go for it .\n",
            "\n",
            "go get it .\n",
            "\n",
            "go get it .\n",
            "\n",
            "go get it .\n",
            "\n",
            "go inside .\n",
            "\n",
            "go to bed .\n",
            "\n",
            "gra| b that .\n",
            "\n",
            "gra| b this .\n",
            "\n",
            "have some .\n",
            "\n",
            "have some .\n",
            "\n",
            "ve .\n",
            "\n",
            "ve| te .\n",
            "\n",
            "va| y| a .\n",
            "\n",
            "va| y| as| e .\n",
            "\n",
            "ho| la .\n",
            "\n",
            "c| or| r| e !\n",
            "\n",
            "c| or| re| d .\n",
            "\n",
            "多 qu| i| en ?\n",
            "\n",
            "f| ue| g| o !\n",
            "\n",
            "in| c| en| di| o !\n",
            "\n",
            "di| sp| ar| ad !\n",
            "\n",
            "ayud| a !\n",
            "\n",
            "so| c| or| r| o ! au| x| i| li| o !\n",
            "\n",
            "au| x| i| li| o !\n",
            "\n",
            "s| al| t| a !\n",
            "\n",
            "s| al| te .\n",
            "\n",
            "p| ar| ad !\n",
            "\n",
            "p| ar| a !\n",
            "\n",
            "p| ar| e !\n",
            "\n",
            "es| p| er| a !\n",
            "\n",
            "es| p| er| en .\n",
            "\n",
            "c| on| t| in| u| a .\n",
            "\n",
            "continu| e .\n",
            "\n",
            "ho| la .\n",
            "\n",
            "c| or| ri .\n",
            "\n",
            "c| or| ria .\n",
            "\n",
            "l| o inten| t| o .\n",
            "\n",
            "he g| an| a| do !\n",
            "\n",
            "o| h , no !\n",
            "\n",
            "tom| at| el| o c| on so| d| a .\n",
            "\n",
            "s| on| ri| e .\n",
            "\n",
            "al a| ta| que !\n",
            "\n",
            "a| ta| c| ad !\n",
            "\n",
            "le| v| an| t| a .\n",
            "\n",
            "ve ahora mism| o .\n",
            "\n",
            "l| o t| en| g| o !\n",
            "\n",
            "多 l| o pi| ll| as ?\n",
            "\n",
            "多 en| t| en| di| s| te ?\n",
            "\n",
            "el c| or| rio .\n",
            "\n",
            "me| te| te ad| entr| o .\n",
            "\n",
            "ab| ra| z| am| e .\n",
            "\n",
            "me ca| i .\n",
            "\n",
            "y| o l| o s| e .\n",
            "\n",
            "s| al| i .\n",
            "\n",
            "m| en| ti .\n",
            "\n",
            "p| er| di .\n",
            "\n",
            "di| mi| t| o .\n",
            "\n",
            "r| en| un| ci| e .\n",
            "\n",
            "es| to| y trabaj| an| do .\n",
            "\n",
            "t| en| g| o di| e| ci| nuev| e .\n",
            "\n",
            "es| to| y le| v| an| tado .\n",
            "\n",
            "escu| ch| a .\n",
            "\n",
            "escu| che .\n",
            "\n",
            "escu| ch| en .\n",
            "\n",
            "no puede s| er !\n",
            "\n",
            "de n| ing| un| a m| an| er| a .\n",
            "\n",
            "de n| ing| un| a m| an| er| a !\n",
            "\n",
            "imp| os| i| bl| e !\n",
            "\n",
            "de n| ing| un mo| do !\n",
            "\n",
            "de es| o nada !\n",
            "\n",
            "n| i ca| g| an| do !\n",
            "\n",
            "m| an| g| os !\n",
            "\n",
            "mi| ng| a !\n",
            "\n",
            "n| i en pe| do !\n",
            "\n",
            "多 en s| er| i| o ?\n",
            "\n",
            "多 l| a verdad ?\n",
            "\n",
            "gra| ci| as !\n",
            "\n",
            "gra| ci| as .\n",
            "\n",
            "pr| ue| b| al| o .\n",
            "\n",
            "l| o pro| cu| ra| m| os .\n",
            "\n",
            "g| an| amos .\n",
            "\n",
            "多 p| or que y| o ?\n",
            "\n",
            "pregunt| al| e a tom .\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "m| an| t| en| te en c| al| m| a .\n",
            "\n",
            "est| ate tran| qu| il| o .\n",
            "\n",
            "s| e ju| sto .\n",
            "\n",
            "se| an g| enti| l| es .\n",
            "\n",
            "s| e a| gra| d| ab| l| e .\n",
            "\n",
            "pi| ra| te .\n",
            "\n",
            "lla| ma| m| e .\n",
            "\n",
            "lla| ma| d| m| e .\n",
            "\n",
            "lla| ma| m| e .\n",
            "\n",
            "llam| an| os .\n",
            "\n",
            "entr| e .\n",
            "\n",
            "pas| e .\n",
            "\n",
            "entr| en !\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "an| d| al| e .\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "su| el| t| al| o .\n",
            "\n",
            "a| g| ar| r| a a tom .\n",
            "\n",
            "ba| ja| te .\n",
            "\n",
            "s| al| te .\n",
            "\n",
            "s| al .\n",
            "\n",
            "s| al| i .\n",
            "\n",
            "s| al| id .\n",
            "\n",
            "s| al| g| an .\n",
            "\n",
            "ve| te de aqui !\n",
            "\n",
            "l| ar| ga| te !\n",
            "\n",
            "s| al| g| a de aqui !\n",
            "\n",
            "l| ar| g| o !\n",
            "\n",
            "ve| te y| a !\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "a l| a c| al| l| e !\n",
            "\n",
            "ve| te de aqui !\n",
            "\n",
            "l| ar| ga| te !\n",
            "\n",
            "l| ar| g| o !\n",
            "\n",
            "ve| te y| a !\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "l| ar| gu| es| e .\n",
            "\n",
            "va| y| as| e .\n",
            "\n",
            "ve| te a casa .\n",
            "\n",
            "va| y| a d| es| pa| ci| o .\n",
            "\n",
            "hasta l| ue| g| o !\n",
            "\n",
            "hasta l| a vi| sta .\n",
            "\n",
            "c| ha| u !\n",
            "\n",
            "es| p| er| a !\n",
            "\n",
            "es| p| er| a un m| om| en| t| o !\n",
            "\n",
            "un se| g| un| do !\n",
            "\n",
            "a| g| ar| r| a fuer| te| m| en| te .\n",
            "\n",
            "el vi| no .\n",
            "\n",
            "el r| en| un| ci| o .\n",
            "\n",
            "ay| u| dam| e .\n",
            "\n",
            "ay| u| dam| e .\n",
            "\n",
            "ec| ha| m| e un| a m| an| o .\n",
            "\n",
            "ay| u| dam| e .\n",
            "\n",
            "ayud| an| os .\n",
            "\n",
            "go| l| pe| a a tom .\n",
            "\n",
            "es| p| er| a .\n",
            "\n",
            "r| es| i| sta .\n",
            "\n",
            "r| es| is| te .\n",
            "\n",
            "a| g| ar| r| a fuer| te| m| en| te .\n",
            "\n",
            "ab| ra| z| a a tom .\n",
            "\n",
            "es| to| y de a| cuer| do .\n",
            "\n",
            "de a| cuer| do .\n",
            "\n",
            "me in| c| l| in| e .\n",
            "\n",
            "me he mu| da| do .\n",
            "\n",
            "me mu| de .\n",
            "\n",
            "me tra| s| la| de .\n",
            "\n",
            "me he tra| s| la| da| do .\n",
            "\n",
            "d| or| mi .\n",
            "\n",
            "l| o inten| te .\n",
            "\n",
            "ir| e .\n",
            "\n",
            "so| y tom .\n",
            "\n",
            "es| to| y g| or| do .\n",
            "\n",
            "so| y g| or| d| a .\n",
            "\n",
            "es| to| y en f| or| m| a .\n",
            "\n",
            "es| to| y a| f| ec| tado .\n",
            "\n",
            "so| y v| ie| j| o .\n",
            "\n",
            "so| y ti| mi| do .\n",
            "\n",
            "es| to| y mo| j| ada .\n",
            "\n",
            "est| a bien .\n",
            "\n",
            "so| y y| o .\n",
            "\n",
            "so| y y| o .\n",
            "\n",
            "un| e| te a n| osotros .\n",
            "\n",
            "s| e p| ar| te nuestr| a .\n",
            "\n",
            "gu| ar| d| al| o .\n",
            "\n",
            "y| o , tam| bien .\n",
            "\n",
            "ab| r| e .\n",
            "\n",
            "p| er| f| ec| t| o !\n",
            "\n",
            "n| os ve| m| os .\n",
            "\n",
            "mu| est| ra| m| el| o .\n",
            "\n",
            "en| s| en| am| el| o .\n",
            "\n",
            "mo| st| ra| m| e .\n",
            "\n",
            "ci| er| r| a l| a bo| c| a !\n",
            "\n",
            "c| er| r| a el p| ico !\n",
            "\n",
            "s| al| ta| t| el| o .\n",
            "\n",
            "hasta l| a vi| sta .\n",
            "\n",
            "hasta l| ue| g| o .\n",
            "\n",
            "p| ar| al| o .\n",
            "\n",
            "de| t| en| l| o .\n",
            "\n",
            "co| g| el| o .\n",
            "\n",
            "deci| m| e .\n",
            "\n",
            "tom comi| o .\n",
            "\n",
            "tom c| or| rio .\n",
            "\n",
            "tom g| an| o .\n",
            "\n",
            "es| p| er| am| e d| es| pi| er| t| o .\n",
            "\n",
            "d| es| pi| er| t| a !\n",
            "\n",
            "d| es| pi| er| ta| te !\n",
            "\n",
            "d| es| p| er| ta| te !\n",
            "\n",
            "d| es| pi| er| t| a .\n",
            "\n",
            "la| va| te la| s m| an| os .\n",
            "\n",
            "n| os pre| ocu| pa| m| os .\n",
            "\n",
            "l| o s| ab| emos .\n",
            "\n",
            "p| er| di| m| os .\n",
            "\n",
            "bi| en| v| en| id| os .\n",
            "\n",
            "bi| en| v| en| id| as .\n",
            "\n",
            "多 qu| i| en comi| o ?\n",
            "\n",
            "多 qu| i| en c| or| ria ?\n",
            "\n",
            "多 qu| i| en c| or| rio ?\n",
            "\n",
            "多 qu| i| en g| an| o ?\n",
            "\n",
            "多 qu| i| en h| a g| an| a| do ?\n",
            "\n",
            "多 p| or que no ?\n",
            "\n",
            "c| or| r| e .\n",
            "\n",
            "has g| an| a| do .\n",
            "\n",
            "多 es| to| y g| or| do ?\n",
            "\n",
            "pregunt| al| es .\n",
            "\n",
            "pregunt| al| es .\n",
            "\n",
            "ap| ar| t| a .\n",
            "\n",
            "ap| ar| ta| te .\n",
            "\n",
            "s| e un hombre .\n",
            "\n",
            "s| e fuer| te .\n",
            "\n",
            "s| e b| re| v| e .\n",
            "\n",
            "sea b| re| v| e .\n",
            "\n",
            "se| an b| re| v| es .\n",
            "\n",
            "est| ate qu| ie| t| o .\n",
            "\n",
            "no te m| uev| as .\n",
            "\n",
            "llam| al| o a tom| as !\n",
            "\n",
            "llam| al| o a tom| as !\n",
            "\n",
            "llam| en| l| o a tom| as !\n",
            "\n",
            "an| i| ma| te .\n",
            "\n",
            "v| en| g| a .\n",
            "\n",
            "c| al| ma| te !\n",
            "\n",
            "es| p| os| al| e .\n",
            "\n",
            "no te va| y| as .\n",
            "\n",
            "c| on| t| in| u| a .\n",
            "\n",
            "encontr| al| o a tom| as .\n",
            "\n",
            "en| cu| entr| al| o a tom| as .\n",
            "\n",
            "en| cu| entr| el| o a tom| as .\n",
            "\n",
            "en| cu| entr| en| l| o a tom| as .\n",
            "\n",
            "en| cu| entr| e a tom .\n",
            "\n",
            "en| cu| entr| en a tom .\n",
            "\n",
            "ar| reg| la est| o .\n",
            "\n",
            "ve| te de aqui !\n",
            "\n",
            "l| ar| ga| te !\n",
            "\n",
            "l| ar| g| o !\n",
            "\n",
            "ve| te y| a !\n",
            "\n",
            "p| on| te a el| l| o .\n",
            "\n",
            "abaj| o .\n",
            "\n",
            "tu| m| ba| te .\n",
            "\n",
            "pi| er| de| te !\n",
            "\n",
            "ve| te de aqui !\n",
            "\n",
            "l| ar| ga| te !\n",
            "\n",
            "l| ar| g| o !\n",
            "\n",
            "ve| te y| a !\n",
            "\n",
            "es| fu| ma| te .\n",
            "\n",
            "d| es| pi| er| t| a !\n",
            "\n",
            "piens| al| o bien !\n",
            "\n",
            "ab| r| e lo| s oj| os .\n",
            "\n",
            "ad| el| an| te .\n",
            "\n",
            "ad| el| an| te .\n",
            "\n",
            "entr| e .\n",
            "\n",
            "entr| a .\n",
            "\n",
            "entr| en .\n",
            "\n",
            "entr| a .\n",
            "\n",
            "en| tra| d .\n",
            "\n",
            "bu| en trabajo !\n",
            "\n",
            "su| je| t| a a tom .\n",
            "\n",
            "a| g| ar| r| al| o .\n",
            "\n",
            "di| vi| er| t| an| s| e .\n",
            "\n",
            "pas| a| la bien .\n",
            "\n",
            "pas| en| la bien .\n",
            "\n",
            "el h| ab| l| o .\n",
            "\n",
            "el l| o inten| t| a .\n",
            "\n",
            "el pr| ue| b| a .\n",
            "\n",
            "ayud| a a tom .\n",
            "\n",
            "ayud| al| o .\n",
            "\n",
            "ho| la , 多 que ha| y ?\n",
            "\n",
            "e| y , c| ha| v| al| es .\n",
            "\n",
            "多 que pas| a , tr| on| c| os ?\n",
            "\n",
            "buen| as .\n",
            "\n",
            "多 que t| al os v| a ?\n",
            "\n",
            "ho| la p| or aqui .\n",
            "\n",
            "que en| c| an| t| o !\n",
            "\n",
            "多 que t| an prof| un| do ?\n",
            "\n",
            "多 como de prof| un| do ?\n",
            "\n",
            "ha| z| m| e el favor .\n",
            "\n",
            "comp| la| ce| m| e .\n",
            "\n",
            "date pri| s| a !\n",
            "\n",
            "a| pr| es| u| ra| te .\n",
            "\n",
            "c| on| v| in| e .\n",
            "\n",
            "a| ce| p| te .\n",
            "\n",
            "ac| ce| di .\n",
            "\n",
            "so| y g| or| d| a .\n",
            "\n",
            "es| to| y v| ie| j| o .\n",
            "\n",
            "me l| o comi .\n",
            "\n",
            "me l| a comi .\n",
            "\n",
            "puedo ir .\n",
            "\n",
            "l| o h| ic| e bien .\n",
            "\n",
            "l| o h| ic| e .\n",
            "\n",
            "f| ra| cas| e .\n",
            "\n",
            "l| o ol| vi| de .\n",
            "\n",
            "me la| s ar| reg| l| o .\n",
            "\n",
            "l| o en| tien| do .\n",
            "\n",
            "l| o t| en| g| o .\n",
            "\n",
            "llam| e p| or t| el| e| f| on| o .\n",
            "\n",
            "me n| ie| g| o .\n",
            "\n",
            "di| mi| t| o .\n",
            "\n",
            "r| en| un| ci| o .\n",
            "\n",
            "l| o v| i .\n",
            "\n",
            "s| on| re| i .\n",
            "\n",
            "me qu| e| de .\n",
            "\n",
            "h| ab| l| e .\n",
            "\n",
            "h| ab| l| ab| a .\n",
            "\n",
            "ch| ar| l| e .\n",
            "\n",
            "ch| ar| l| ab| a .\n",
            "\n",
            "y| o l| o us| o .\n",
            "\n",
            "es| p| er| e .\n",
            "\n",
            "y| o pa| g| ar| e .\n",
            "\n",
            "he vuel| t| o .\n",
            "\n",
            "es| to| y de vuel| t| a .\n",
            "\n",
            "so| y c| al| v| o .\n",
            "\n",
            "es| to| y c| al| v| o .\n",
            "\n",
            "es| to| y c| al| ma| do .\n",
            "\n",
            "es| to| y tran| qu| il| o .\n",
            "\n",
            "he t| er| mi| na| do .\n",
            "\n",
            "so| y ba| st| an| te fa| ci| l| on .\n",
            "\n",
            "so| y ju| sto .\n",
            "\n",
            "es| to| y p| er| f| ec| tamen| te .\n",
            "\n",
            "so| y li| bre !\n",
            "\n",
            "y| o so| y li| bre .\n",
            "\n",
            "es| to| y ll| en| o .\n",
            "\n",
            "es| to| y ll| en| a .\n",
            "\n",
            "y| a me ll| en| e .\n",
            "\n",
            "es| to| y aqui .\n",
            "\n",
            "es| to| y en casa .\n",
            "\n",
            "es| to| y h| er| i| do .\n",
            "\n",
            "lleg| o t| ar| de .\n",
            "\n",
            "so| y va| g| o .\n",
            "\n",
            "es| to| y p| er| di| d| a .\n",
            "\n",
            "so| y m| al| o .\n",
            "\n",
            "me to| c| a a mi .\n",
            "\n",
            "es| to| y p| er| f| ec| tamen| te .\n",
            "\n",
            "so| y po| bre .\n",
            "\n",
            "so| y ri| c| a .\n",
            "\n",
            "so| y ri| c| o .\n",
            "\n",
            "es| to| y a s| al| v| o .\n",
            "\n",
            "es| to| y en| f| er| m| a .\n",
            "\n",
            "so| y d| el| ga| do .\n",
            "\n",
            "es| to| y li| mp| i| o .\n",
            "\n",
            "es| to| y c| al| enti| t| o .\n",
            "\n",
            "es| to| y de| bi| l .\n",
            "\n",
            "so| y li| sto .\n",
            "\n",
            "g| an| e .\n",
            "\n",
            "es| o ayud| a .\n",
            "\n",
            "du| el| e .\n",
            "\n",
            "f| un| ci| on| a .\n",
            "\n",
            "es tom .\n",
            "\n",
            "es di| v| er| ti| do .\n",
            "\n",
            "es su| y| o .\n",
            "\n",
            "es nuevo .\n",
            "\n",
            "es ex| tran| o .\n",
            "\n",
            "es v| ie| j| o .\n",
            "\n",
            "es ro| j| o .\n",
            "\n",
            "es t| ri| s| te .\n",
            "\n",
            "pro| hi| bi| do pas| ar .\n",
            "\n",
            "no entr| ar .\n",
            "\n",
            "b| es| a a tom| as .\n",
            "\n",
            "b| es| a a tom| as .\n",
            "\n",
            "b| es| en a tom| as .\n",
            "\n",
            "dej| al| o .\n",
            "\n",
            "dej| am| e .\n",
            "\n",
            "dej| an| os .\n",
            "\n",
            "v| amos .\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "c| ui| da| do .\n",
            "\n",
            "cas| a| te c| on| mi| g| o .\n",
            "\n",
            "多 puedo ir ?\n",
            "\n",
            "s| al| v| a a tom .\n",
            "\n",
            "el| la vi| no .\n",
            "\n",
            "el| la mu| rio .\n",
            "\n",
            "el| la c| or| r| e .\n",
            "\n",
            "s| en| ta| te !\n",
            "\n",
            "sien| ta| te .\n",
            "\n",
            "sien| ta| te aqui .\n",
            "\n",
            "h| ab| l| e m| as fuer| te !\n",
            "\n",
            "h| ab| la m| as fuer| te .\n",
            "\n",
            "h| ab| la m| as al| t| o !\n",
            "\n",
            "pre| p| ar| a| te .\n",
            "\n",
            "un m| om| en| t| o .\n",
            "\n",
            "p| ar| a| te !\n",
            "\n",
            "de pi| e !\n",
            "\n",
            "no te m| uev| as .\n",
            "\n",
            "de| t| en a tom .\n",
            "\n",
            "lle| va| te a tom .\n",
            "\n",
            "di| s| el| o a tom .\n",
            "\n",
            "g| en| i| al !\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "el| l| os g| an| ar| on .\n",
            "\n",
            "tom vi| no .\n",
            "\n",
            "tom h| a mu| er| t| o .\n",
            "\n",
            "tom s| e ca| y| o .\n",
            "\n",
            "tom l| o s| ab| i| a .\n",
            "\n",
            "tom t| en| i| a c| on| st| an| ci| a de el| l| o .\n",
            "\n",
            "tom s| e fue .\n",
            "\n",
            "tom mi| nt| i| o .\n",
            "\n",
            "tom| as mi| en| te .\n",
            "\n",
            "tom p| er| di| o .\n",
            "\n",
            "tom pa| g| o .\n",
            "\n",
            "tom r| en| un| ci| o .\n",
            "\n",
            "tom na| do .\n",
            "\n",
            "tom ll| or| o .\n",
            "\n",
            "tom s| e h| a le| v| an| tado .\n",
            "\n",
            "demasiado t| ar| de .\n",
            "\n",
            "c| on| fi| a en mi .\n",
            "\n",
            "inten| t| al| o de v| er| as .\n",
            "\n",
            "pr| ue| b| a un poco .\n",
            "\n",
            "pr| ue| b| e un poco .\n",
            "\n",
            "pr| ue| b| en un poco .\n",
            "\n",
            "pr| ue| b| en est| o .\n",
            "\n",
            "prob| a est| o .\n",
            "\n",
            "pr| ue| b| a est| o .\n",
            "\n",
            "u| s| a est| o .\n",
            "\n",
            "a| vi| s| al| e a tom .\n",
            "\n",
            "a| vi| s| el| e a tom .\n",
            "\n",
            "a| vi| s| en| l| e a tom .\n",
            "\n",
            "a| vi| s| al| e a tom .\n",
            "\n",
            "mi| ra| m| e .\n",
            "\n",
            "vi| gi| la| m| e .\n",
            "\n",
            "ob| s| er| v| am| e .\n",
            "\n",
            "ob| s| er| v| al| os .\n",
            "\n",
            "ob| s| er| v| an| os .\n",
            "\n",
            "est| amos de a| cuer| do .\n",
            "\n",
            "l| o inten| tam| os .\n",
            "\n",
            "i| re| m| os .\n",
            "\n",
            "est| amos bien .\n",
            "\n",
            "多 p| ar| a que ?\n",
            "\n",
            "que di| v| er| ti| do !\n",
            "\n",
            "多 qu| i| en so| y y| o ?\n",
            "\n",
            "多 qu| i| en vi| no ?\n",
            "\n",
            "多 qu| i| en mu| rio ?\n",
            "\n",
            "多 qu| i| en s| e c| al| l| o ?\n",
            "\n",
            "多 qu| i| en s| e ca| y| o ?\n",
            "\n",
            "多 qu| i| en s| e h| a i| do ?\n",
            "\n",
            "多 qu| i| en l| o h| a de| ja| do ?\n",
            "\n",
            "多 qu| i| en na| do ?\n",
            "\n",
            "多 qu| i| en es el ?\n",
            "\n",
            "escri| be| m| e .\n",
            "\n",
            "d| espu| es de ti .\n",
            "\n",
            "tu prim| er| o .\n",
            "\n",
            "usted prim| er| o .\n",
            "\n",
            "d| espu| es de usted .\n",
            "\n",
            "d| espu| es de v| os| otr| as .\n",
            "\n",
            "ap| un| t| a . f| ue| g| o !\n",
            "\n",
            "r| es| p| on| dem| e .\n",
            "\n",
            "r| es| p| on| de| d| m| e .\n",
            "\n",
            "r| es| p| on| d| an| m| e .\n",
            "\n",
            "lo| s pa| j| ar| os vuel| an .\n",
            "\n",
            "je| su| s .\n",
            "\n",
            "lla| ma a casa !\n",
            "\n",
            "c| al| ma| te .\n",
            "\n",
            "tran| qu| i| li| za| te .\n",
            "\n",
            "tom| at| el| o c| on so| d| a .\n",
            "\n",
            "ba| j| a un cam| bi| o .\n",
            "\n",
            "多 po| dem| os ir ?\n",
            "\n",
            "多 po| dem| os ir| n| os ?\n",
            "\n",
            "多 po| dem| os m| ar| ch| ar| n| os ?\n",
            "\n",
            "a| tra| p| el| o a tom| as !\n",
            "\n",
            "ca| p| tu| r| al| o .\n",
            "\n",
            "ca| p| tu| r| en| l| o .\n",
            "\n",
            "a| tra| p| en| l| o .\n",
            "\n",
            "a| tra| p| al| o .\n",
            "\n",
            "vuel| v| e .\n",
            "\n",
            "v| en| i .\n",
            "\n",
            "v| en .\n",
            "\n",
            "v| en| id aqui .\n",
            "\n",
            "v| en a casa .\n",
            "\n",
            "v| en| id aqui .\n",
            "\n",
            "v| en| te .\n",
            "\n",
            "v| en| i| os .\n",
            "\n",
            "v| en| g| a aqui .\n",
            "\n",
            "vuel| v| e pr| on| t| o .\n",
            "\n",
            "ha| z| l| o ahora .\n",
            "\n",
            "lo| s p| er| r| os la| dr| an .\n",
            "\n",
            "no pregunt| es .\n",
            "\n",
            "no ll| or| es .\n",
            "\n",
            "no ll| or| es !\n",
            "\n",
            "no ll| or| es !\n",
            "\n",
            "no ll| or| en .\n",
            "\n",
            "no mi| n| ta| i| s .\n",
            "\n",
            "no c| or| r| as .\n",
            "\n",
            "f| an| ta| st| ico !\n",
            "\n",
            "es est| u| p| en| do !\n",
            "\n",
            "or| al| e !\n",
            "\n",
            "tien| t| a est| o .\n",
            "\n",
            "tien| te est| o .\n",
            "\n",
            "tien| t| en est| o .\n",
            "\n",
            "t| en| tad est| o .\n",
            "\n",
            "t| en| t| a est| o .\n",
            "\n",
            "si| g| ue| m| e .\n",
            "\n",
            "se| gui| d| m| e .\n",
            "\n",
            "si| gu| en| os .\n",
            "\n",
            "dej| al| o .\n",
            "\n",
            "ol| vi| dam| e .\n",
            "\n",
            "ol| vi| da| te de mi .\n",
            "\n",
            "o| lv| id| en| m| e .\n",
            "\n",
            "o| lv| id| en| s| e de mi .\n",
            "\n",
            "pre| p| ar| a| os .\n",
            "\n",
            "ve a p| or el| l| o .\n",
            "\n",
            "ve a co| g| er| l| o .\n",
            "\n",
            "ve a p| or el| l| o .\n",
            "\n",
            "ve| te a p| or el| l| o .\n",
            "\n",
            "id a p| or el| l| o .\n",
            "\n",
            "entr| a .\n",
            "\n",
            "ve| te a l| a cam| a .\n",
            "\n",
            "co| g| el| o .\n",
            "\n",
            "a| g| ar| r| a est| o .\n",
            "\n",
            "co| g| e al| g| un| o .\n",
            "\n",
            "tom| a al| g| o .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixXpJhYbrRMy"
      },
      "source": [
        "Replacing original dataset with \"subworded\" dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id-2GWw1rPca",
        "outputId": "c43b7f42-d1a1-4dc1-cdf1-397dffb39898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def subworded_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  x = [preprocess_sentence2(l) for l in lines[:num_examples]]\n",
        "  \n",
        "  return x\n",
        "\n",
        "def subworded_dataset2(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  x = [preprocess_sentence(l) for l in lines[:num_examples]]\n",
        "  \n",
        "  return x\n",
        "\n",
        "en = subworded_dataset(\"output_en.txt\", 10000)\n",
        "sp = subworded_dataset(\"output_sp.txt\", 10000)\n",
        "\n",
        "print(en[5],sp[5])\n",
        "print(en2[5],sp2[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> run ! <end> <start> c or r e ! <end>\n",
            "run ! corre !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti7jGQwQwr_t"
      },
      "source": [
        "The seq2seq with attention translation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GusLzOZSwq42",
        "outputId": "a334f09c-f0ba-4fd9-ab79-e7d968053f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "#Change input and output language here\n",
        "input_tensor, inp_lang = tokenize(sp)\n",
        "target_tensor, targ_lang = tokenize(en)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 8000 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKhcH7vp0J6K",
        "outputId": "d38392b8-6e76-4b86-853b-7211179fc271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "61 ----> tu\n",
            "16 ----> c\n",
            "28 ----> on\n",
            "183 ----> oc\n",
            "6 ----> es\n",
            "33 ----> g\n",
            "8 ----> en\n",
            "25 ----> te\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "6 ----> you\n",
            "47 ----> know\n",
            "612 ----> people\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo6PVjsf1piV",
        "outputId": "2fba1326-0cb2-47cb-b7c3-8941738ec662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 80\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([80, 22]), TensorShape([80, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xuDNKPI2aAA"
      },
      "source": [
        "Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69hs60N62T2f",
        "outputId": "75271fdc-6706-4190-a1a2-82a87dfcf288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (80, 22, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (80, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BidoFsJ03dNM",
        "outputId": "4fab05c8-b229-4d3d-8f77-02c64f32e237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (80, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (80, 22, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q4IcIPr3w1N"
      },
      "source": [
        "Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsJ2Kooi3zac",
        "outputId": "af92f9a7-b972-4b69-9e15-8d629e3d9140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (80, 1701)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N36JAOKl4SdI"
      },
      "source": [
        "Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lMpwjfF4K7z"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adadelta()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THexZWGx54NI"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pebXIs53BH"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5fGCGyq6H37",
        "outputId": "d2fd7a16-627e-42f0-c270-da743f9f0ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 20 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.7023\n",
            "Epoch 1 Batch 20 Loss 3.9047\n",
            "Epoch 1 Batch 40 Loss 3.7948\n",
            "Epoch 1 Batch 60 Loss 3.8875\n",
            "Epoch 1 Batch 80 Loss 3.7265\n",
            "Epoch 1 Loss 3.8018\n",
            "Time taken for 1 epoch 373.4365780353546 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.7855\n",
            "Epoch 2 Batch 20 Loss 3.8275\n",
            "Epoch 2 Batch 40 Loss 4.0554\n",
            "Epoch 2 Batch 60 Loss 3.8100\n",
            "Epoch 2 Batch 80 Loss 3.7928\n",
            "Epoch 2 Loss 3.8005\n",
            "Time taken for 1 epoch 358.6863694190979 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.7671\n",
            "Epoch 3 Batch 20 Loss 3.8602\n",
            "Epoch 3 Batch 40 Loss 3.7923\n",
            "Epoch 3 Batch 60 Loss 3.6987\n",
            "Epoch 3 Batch 80 Loss 3.7831\n",
            "Epoch 3 Loss 3.7991\n",
            "Time taken for 1 epoch 359.6864058971405 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.7321\n",
            "Epoch 4 Batch 20 Loss 3.8842\n",
            "Epoch 4 Batch 40 Loss 3.7146\n",
            "Epoch 4 Batch 60 Loss 3.9001\n",
            "Epoch 4 Batch 80 Loss 3.8914\n",
            "Epoch 4 Loss 3.7977\n",
            "Time taken for 1 epoch 358.653746843338 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.7726\n",
            "Epoch 5 Batch 20 Loss 3.8489\n",
            "Epoch 5 Batch 40 Loss 3.8651\n",
            "Epoch 5 Batch 60 Loss 3.7296\n",
            "Epoch 5 Batch 80 Loss 3.7972\n",
            "Epoch 5 Loss 3.7961\n",
            "Time taken for 1 epoch 360.05248188972473 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.8048\n",
            "Epoch 6 Batch 20 Loss 3.7959\n",
            "Epoch 6 Batch 40 Loss 3.8719\n",
            "Epoch 6 Batch 60 Loss 3.7448\n",
            "Epoch 6 Batch 80 Loss 3.7108\n",
            "Epoch 6 Loss 3.7945\n",
            "Time taken for 1 epoch 359.64138984680176 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 3.8706\n",
            "Epoch 7 Batch 20 Loss 3.7947\n",
            "Epoch 7 Batch 40 Loss 3.7603\n",
            "Epoch 7 Batch 60 Loss 3.6586\n",
            "Epoch 7 Batch 80 Loss 3.7679\n",
            "Epoch 7 Loss 3.7927\n",
            "Time taken for 1 epoch 359.15483117103577 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 3.7935\n",
            "Epoch 8 Batch 20 Loss 3.8103\n",
            "Epoch 8 Batch 40 Loss 3.7922\n",
            "Epoch 8 Batch 60 Loss 3.8173\n",
            "Epoch 8 Batch 80 Loss 3.7150\n",
            "Epoch 8 Loss 3.7909\n",
            "Time taken for 1 epoch 358.75277185440063 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.6812\n",
            "Epoch 9 Batch 20 Loss 3.8417\n",
            "Epoch 9 Batch 40 Loss 3.8240\n",
            "Epoch 9 Batch 60 Loss 3.8489\n",
            "Epoch 9 Batch 80 Loss 3.6964\n",
            "Epoch 9 Loss 3.7889\n",
            "Time taken for 1 epoch 359.1123502254486 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.7809\n",
            "Epoch 10 Batch 20 Loss 3.7977\n",
            "Epoch 10 Batch 40 Loss 3.7720\n",
            "Epoch 10 Batch 60 Loss 3.8296\n",
            "Epoch 10 Batch 80 Loss 3.7789\n",
            "Epoch 10 Loss 3.7868\n",
            "Time taken for 1 epoch 359.37255668640137 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5OGeG277Xa_"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUx0RIKu7WzQ"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence2(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJlMLAcd-Pmq"
      },
      "source": [
        "Plotting Attention Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1LupJgP-Owc"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtOA3yqB-ZDe"
      },
      "source": [
        "Translating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKWINuli-Yo5",
        "outputId": "830a8991-3344-4f55-bd12-56e57fc467af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  # print('Input: %s' % (sentence))\n",
        "  # print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "  return result\n",
        "\n",
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f83ec612b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE4EzodzGg87",
        "outputId": "07dcbd08-052e-4fc7-a572-1e93aca1020a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "dset = sp2[3000:4000]\n",
        "\n",
        "\n",
        "\n",
        "eval_codes = open(\"codes_sp.txt\" , \"r\")\n",
        "\n",
        "eval_out = open(\"output_eval.txt\" , \"w\")\n",
        "seperator_eval = '|'\n",
        "bpe_eval = BPE(eval_codes, seperator_eval)\n",
        "\n",
        "for line in dset:\n",
        "  eval_out.write(bpe_eval.segment(line).strip())\n",
        "  eval_out.write('\\n')\n",
        "  # print(line)\n",
        "\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "eval_out.close()\n",
        "\n",
        "t_eval_out = open(\"output_eval.txt\" , \"r\")\n",
        "\n",
        "i = 0\n",
        "total_score = 0\n",
        "\n",
        "for line in t_eval_out:\n",
        "  candidate = str(translate(line))\n",
        "  # candidate = candidate.split()\n",
        "  reference = str(en2[i])\n",
        "  # reference = reference.split()\n",
        "  score = sentence_bleu(candidate, reference, weights=(1, 0, 0, 0))\n",
        "  total_score = total_score + score\n",
        "  # print(score)\n",
        "  # print(\"--------------------------------\")\n",
        "  i = i+1\n",
        "\n",
        "print(\"--------------------------------\\nAVERAGE BLEU SCORE \\n--------------------------------\")\n",
        "print(total_score/(i+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------\n",
            "AVERAGE BLEU SCORE \n",
            "--------------------------------\n",
            "0.20140404328716144\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}