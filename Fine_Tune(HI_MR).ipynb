{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune(HI-MR)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLfx2cobDIYN"
      },
      "source": [
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGoqKPZ4YJDe",
        "outputId": "e7fe1078-447d-4950-cdf0-078c22ccdc19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "!pip install inltk\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import inltk\n",
        "from inltk.inltk import tokenize\n",
        "from inltk.inltk import setup\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: inltk in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from inltk) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.18.5)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from inltk) (3.7.4.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from inltk) (2.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from inltk) (7.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from inltk) (20.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.5)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from inltk) (2.2.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from inltk) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from inltk) (0.7)\n",
            "Requirement already satisfied: fastai==1.0.57 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.57)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from inltk) (1.4.1)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from inltk) (1.3.2)\n",
            "Requirement already satisfied: aiohttp>=3.5.4 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.6.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from inltk) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from inltk) (2.23.0)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from inltk) (0.2.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from inltk) (3.2.2)\n",
            "Requirement already satisfied: async-timeout>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.0.1)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from inltk) (7.352.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2.8.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (49.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (0.6.1+cu101)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (1.5.1+cu101)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.7.4.2)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (4.7.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (19.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2.10)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (1.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (1.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.1.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2UbHQwXYTgt"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "#initialize Lang Class\n",
        "class Lang:\n",
        "   def __init__(self):\n",
        "       #initialize containers to hold the words and corresponding index\n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "       self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "#split a sentence into words and add it to the container\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "#If the word is not in the container, the word will be added to it, \n",
        "#else, update the word counter\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJQo1m3cYa27"
      },
      "source": [
        "def process_data():\n",
        "\n",
        "  sf = open('/content/drive/My Drive/data/bible-uedin.hi-mr.hi' , \"r\")\n",
        "  tf = open('/content/drive/My Drive/data/bible-uedin.hi-mr.mr' , \"r\")\n",
        "\n",
        "  source = Lang()\n",
        "  target = Lang()\n",
        "  pairs = []\n",
        "  count = 0\n",
        "  count2 = 0\n",
        "  for sent in sf:\n",
        "    sent = sent.strip()\n",
        "    source.addSentence(sent)\n",
        "    pairs.append(sent)\n",
        "    count = count+1\n",
        "\n",
        "  # print(count)\n",
        "\n",
        "  for sent in tf:\n",
        "    sent = sent.strip()\n",
        "    target.addSentence(sent.strip())\n",
        "    pairs.append(sent)\n",
        "    count2 = count2 + 1\n",
        "\n",
        "  # print(count2)\n",
        "\n",
        "  pairs_new = []\n",
        "\n",
        "  for i in range(count):\n",
        "    full = [pairs[i],pairs[i+count]]\n",
        "    pairs_new.append(full)\n",
        "\n",
        "\n",
        "\n",
        "  return source, target, pairs_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92bJScQFZ3Kz"
      },
      "source": [
        "# source, target, pairs = process_data()   JUST A CHECK TO SEE IF THE SIZE OF BOTH DATASET IS SAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRftbm5OC9Mp"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBdBN2bxdbC"
      },
      "source": [
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gUpG5q-DDop"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOZOdZTjdAWa"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PYQnQWvE0w5"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4xdfq9OdXqG"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tFsKzhQda3T"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBjkaSjjdgDg"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        # plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "    #     if iter % plot_every == 0:\n",
        "    #         plot_loss_avg = plot_loss_total / plot_every\n",
        "    #         plot_losses.append(plot_loss_avg)\n",
        "    #         plot_loss_total = 0\n",
        "\n",
        "    # showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLAP-AQedj8D"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(val_pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNDMkSjdx42",
        "outputId": "78d3e3b9-14ce-4a64-cb0a-ff63d09452ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "input_lang, output_lang, pairs = process_data()\n",
        "pairs = filterPairs(pairs)\n",
        "\n",
        "print(random.choice(pairs))\n",
        "\n",
        "print(len(pairs))\n",
        "x = len(pairs)\n",
        "\n",
        "trainSplit = int(x*0.7)\n",
        "valSplit = trainSplit + int(x*0.2)\n",
        "\n",
        "train_pairs = pairs[:trainSplit]\n",
        "val_pairs = pairs[trainSplit:valSplit]\n",
        "test_pairs = pairs[valSplit:]\n",
        "\n",
        "print(len(train_pairs))\n",
        "print(len(val_pairs))\n",
        "print(len(test_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['और जब से नित्य होमबलि उठाई जाएगी, और वह घिनौनी वस्तु जो उजाड़ करा देती है, स्थापित की जाएगी, तब से बारह सौ नब्बे दिन बीतेंगे।', '“‘नित्याची होमार्पणे करणे बंद होईल. ह्या वेळेपासून ती भयंकर नाश करणारी गोष्टी घेडेपर्यंतचा काळ हा 1290 दिवसांचा असेल.']\n",
            "30381\n",
            "21266\n",
            "6076\n",
            "3039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F5bEVeZzcXW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwctMktLdqQw",
        "outputId": "e5997644-f5df-4e85-91bf-6720baa55d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 30000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 43s (- 78m 53s) (1000 3%) 6.5134\n",
            "5m 26s (- 76m 7s) (2000 6%) 6.5417\n",
            "8m 5s (- 72m 49s) (3000 10%) 6.3418\n",
            "10m 52s (- 70m 41s) (4000 13%) 6.4789\n",
            "13m 36s (- 68m 1s) (5000 16%) 6.3213\n",
            "16m 18s (- 65m 12s) (6000 20%) 6.2228\n",
            "18m 59s (- 62m 25s) (7000 23%) 6.2667\n",
            "21m 45s (- 59m 49s) (8000 26%) 6.3205\n",
            "24m 33s (- 57m 18s) (9000 30%) 6.3930\n",
            "27m 15s (- 54m 31s) (10000 33%) 6.1993\n",
            "30m 4s (- 51m 56s) (11000 36%) 6.3785\n",
            "32m 51s (- 49m 17s) (12000 40%) 6.3317\n",
            "35m 40s (- 46m 39s) (13000 43%) 6.2055\n",
            "38m 30s (- 44m 0s) (14000 46%) 6.2430\n",
            "41m 22s (- 41m 22s) (15000 50%) 6.2570\n",
            "44m 20s (- 38m 47s) (16000 53%) 6.2254\n",
            "47m 11s (- 36m 5s) (17000 56%) 6.3019\n",
            "50m 7s (- 33m 25s) (18000 60%) 6.3244\n",
            "53m 0s (- 30m 41s) (19000 63%) 6.2093\n",
            "55m 55s (- 27m 57s) (20000 66%) 6.2274\n",
            "58m 53s (- 25m 14s) (21000 70%) 6.3411\n",
            "61m 51s (- 22m 29s) (22000 73%) 6.2953\n",
            "64m 46s (- 19m 42s) (23000 76%) 6.1514\n",
            "67m 41s (- 16m 55s) (24000 80%) 6.2219\n",
            "70m 37s (- 14m 7s) (25000 83%) 6.2149\n",
            "73m 33s (- 11m 18s) (26000 86%) 6.1064\n",
            "76m 33s (- 8m 30s) (27000 90%) 6.1913\n",
            "79m 29s (- 5m 40s) (28000 93%) 6.1326\n",
            "82m 23s (- 2m 50s) (29000 96%) 6.0039\n",
            "85m 18s (- 0m 0s) (30000 100%) 6.1757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLSoH_89d5Eg",
        "outputId": "29064017-2918-4efd-ac54-92c7f26419fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> नीकुदेमुस ने, (जो पहिले उसके पास आया था और उन में से एक था), उन से कहा।\n",
            "= परंतु त्या घोळक्यात निकदेम हजार होता. यापूर्वी हा निकदेमच येशूला भेटायला आला होता निकदेम म्हणाला.\n",
            "< मग तो सर्व व व तो व <EOS>\n",
            "\n",
            "> इतने में जब हजारों की भीड़ लग गई, यहां तक कि एक दूसरे पर गिरे पड़ते थे, तो वह सब से पहिले अपने चेलों से कहने लगा, कि फरीसियों के कपटरूपी खमीर से चौकस रहना।\n",
            "= आणि म्हणून हजारो लोकांचा समुदाय जमला होता. इतके लोक जमले होते की, ते एकमेकांना तुडवू लागले, तेव्हा येशू प्रथम आपल्या शिष्यांशी बोलला: “परुश्यांच्या खमिराविषयी जपा, म्हणजे जे ढोंग आहे त्याविषयी जपा.\n",
            "< पण या सर्व सर्व सर्व या सर्व सर्व लोक या सर्व त्यांना ते ते <EOS>\n",
            "\n",
            "> और मैं उन्हें अनन्त जीवन देता हूं, और वे कभी नाश नहीं होंगी, और कोई उन्हें मेरे हाथ से छीन न लेगा।\n",
            "= मी माझ्या मेंढरांना अनंतकाळचे जीवन देतो. ती कधीच मरणार नाहीत. आणि त्यांना कोणीच माझ्या हातून हिरावून घेणार नाही.\n",
            "< मी मी मी आहे. मी मी मी मी मी मी मी मी मी मी मी मी मी मी मी मी मी त्यांना मी मी मी <EOS>\n",
            "\n",
            "> यीशु यरूशलेम को जाते हुए बारह चेलों को एकान्त में ले गया, और मार्ग में उन से कहने लगा।\n",
            "= येशू यरूशलेमला चालला होता. तो चालत असता त्याने त्याच्या शिष्यांना बाजूला घेतले, आणि त्यांना म्हणाला,\n",
            "< पण हा सर्व या सर्व आणि आणि तो आणि तो तो <EOS>\n",
            "\n",
            "> क्योंकि जो कोई अपना प्राण बचाना चाहे वह उसे खोएगा, पर जो कोई मेरे और सुसमाचार के लिये अपना प्राण खोएगा, वह उसे बचाएगा।\n",
            "= जो कोणी आपला जीव वाचवू पाहतो तो जिवाला मुकेल व जो कोणी माइयासाठी व सुवार्तेसाठी जिवाला मुकेल तो आपला जीव वाचवील.\n",
            "< जर एखाद्याने त्याच्या तर तर त्याला मी आहे. तर त्याला मी आहे. आहे. तर मी त्याला मी आहे. आहे. <EOS>\n",
            "\n",
            "> उस ने कहा; जो मनुष्य से नहीं हो सकता, वह परमेश्वर से हो सकता है।\n",
            "= येशू म्हणाला, “ज्या गोष्टी माणसांना अशक्य आहेत त्या देवाला शक्य आहेत.”\n",
            "< पण त्याला त्याला त्याला त्याला त्याला मान तो त्याला तो <EOS>\n",
            "\n",
            "> फिर तुम सुन चुके हो, कि पूर्वकाल के लोगों से कहा गया था कि झूठी शपथ न खाना, परन्तु प्रभु के लिये अपनी शपथ को पूरी करना।\n",
            "= “जेव्हा तू शपथ घेतोस तेव्हा ती तोडू नको. जी शपथ देवाला वाहिली आहे ती खरी कर, असे सांगितल्याचे तुम्ही ऐकले आहे.\n",
            "< पण जर तुम्ही तुमच्या तर तर तर तर तर तर तर तुम्ही तर तुम्ही तर तुम्ही तर तुम्ही तुम्ही तर तुम्ही तुम्ही तर तुम्ही तुम्ही तुम्ही तुम्ही तुम्ही <EOS>\n",
            "\n",
            "> तब बकरा अत्यन्त बड़ाई मारने लगा, और जब बलवन्त हुआ, तक उसका बड़ा सींग टूट गया, और उसकी सन्ती देखने योग्य चार सींग निकलकर चारों दिशाओं की ओर बढ़ने लगे।।\n",
            "= मग बोकड खूपच शक्तिशाली झाला, पण तो बलिष्ठ होताच त्याचे मोठे शिंग मोडले व त्या जागी चार शिंगे उगवली. ती सहज दिसण्यासारखी होती. त्यांची टोके चार दिशांना होती.\n",
            "< मग त्याने सर्व व व व तो व तो तो आणि तो <EOS>\n",
            "\n",
            "> क्योंकि अब मैं उसका जूआ तेरी गर्दन पर से उतारकर तोड़ डालूंगा, और तेरा बन्धन फाड़ डालूंगा।।\n",
            "= आता मी तुमची अश्शूरच्या सत्तेपासून मुक्तता करीन मी तुमच्या मानेवरचे जोखड काढून घेईन तुम्हाला बांधणाव्या साखळ्या मी तोडून टाकीन.\n",
            "< परमेश्वरा, मी तुला मी मी आणि आहे. मी तुला मी आहे. <EOS>\n",
            "\n",
            "> जब वह वहां से निकला, तो शास्त्री और फरीसी बहुत पीछे पड़ गए और छेड़ने लगे, कि वह बहुत सी बातों की चर्चा करे।\n",
            "= येशू तेथून निघून जात असता नियमशास्त्राचे शिक्षक व परुशी फार विरोध करु लागले व त्याला अनेक गोष्टीविषयी प्रश्न विचारु लागले.\n",
            "< मग त्याने सर्व व व व त्याने त्याने त्याने <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLvGEhf0j0C1"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
        "       super().__init__()\n",
        "      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "#encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "#use the encoder’s hidden layer as the decoder hidden\n",
        "       decoder_hidden = encoder_hidden.to(device)\n",
        "  \n",
        "#add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "\n",
        "#topk is used to get the top K value over a list\n",
        "#predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == EOS_token):\n",
        "               break\n",
        "\n",
        "       return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piwtmuh7jDWh"
      },
      "source": [
        "model = Seq2Seq(encoder1, attn_decoder1, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3a93SLXj2gD",
        "outputId": "8315e612-1d07-4429-f0a7-d3caec6912cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, params in model.named_children():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder\n",
            "decoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5faGpUDKlZwZ"
      },
      "source": [
        "for param in model.parameters():    \n",
        "    param.requires_grad = False\n",
        "\n",
        "trained_encoder = list(model.children())[0]\n",
        "trained_decoder = list(model.children())[1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6vf0E0ScfsV",
        "outputId": "61e62788-e224-415d-d673-ded087accd52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder2 = trained_decoder\n",
        "\n",
        "trainIters(encoder2, attn_decoder2, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 41s (- 23m 46s) (1000 6%) 6.3019\n",
            "3m 18s (- 21m 30s) (2000 13%) 6.2118\n",
            "4m 56s (- 19m 45s) (3000 20%) 6.2099\n",
            "6m 35s (- 18m 7s) (4000 26%) 6.2319\n",
            "8m 15s (- 16m 31s) (5000 33%) 6.2526\n",
            "9m 55s (- 14m 53s) (6000 40%) 6.2512\n",
            "11m 34s (- 13m 13s) (7000 46%) 6.1437\n",
            "13m 14s (- 11m 35s) (8000 53%) 6.1928\n",
            "14m 55s (- 9m 56s) (9000 60%) 6.2518\n",
            "16m 33s (- 8m 16s) (10000 66%) 6.1545\n",
            "18m 15s (- 6m 38s) (11000 73%) 6.2133\n",
            "19m 53s (- 4m 58s) (12000 80%) 6.1837\n",
            "21m 33s (- 3m 19s) (13000 86%) 6.2059\n",
            "23m 13s (- 1m 39s) (14000 93%) 5.9653\n",
            "24m 52s (- 0m 0s) (15000 100%) 6.1422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul6xiMFA6YA6",
        "outputId": "7794eaa0-42fe-4efe-bd3c-88fae93a4f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "new_model = Seq2Seq(encoder2, attn_decoder2, device).to(device)\n",
        "\n",
        "for param in new_model.parameters():    \n",
        "    param.requires_grad = True\n",
        "\n",
        "trained_encoder = list(new_model.children())[0]\n",
        "trained_decoder = list(new_model.children())[1] \n",
        "\n",
        "trainIters(trained_encoder, trained_decoder, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3m 0s (- 42m 3s) (1000 6%) 6.1017\n",
            "5m 58s (- 38m 52s) (2000 13%) 6.0436\n",
            "8m 55s (- 35m 42s) (3000 20%) 6.0422\n",
            "11m 51s (- 32m 36s) (4000 26%) 6.1170\n",
            "14m 50s (- 29m 40s) (5000 33%) 5.9608\n",
            "17m 40s (- 26m 31s) (6000 40%) 6.0040\n",
            "20m 36s (- 23m 33s) (7000 46%) 6.0285\n",
            "23m 34s (- 20m 37s) (8000 53%) 6.0565\n",
            "26m 31s (- 17m 40s) (9000 60%) 6.0269\n",
            "29m 29s (- 14m 44s) (10000 66%) 5.9615\n",
            "32m 27s (- 11m 48s) (11000 73%) 5.9305\n",
            "35m 23s (- 8m 50s) (12000 80%) 5.9502\n",
            "38m 19s (- 5m 53s) (13000 86%) 5.9157\n",
            "41m 13s (- 2m 56s) (14000 93%) 5.9355\n",
            "44m 6s (- 0m 0s) (15000 100%) 5.8693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WCaEOcIuVmV"
      },
      "source": [
        "def BLEU_score(encoder, decoder, n = len(test_pairs)):\n",
        "  score = 0\n",
        "  for i in range(n):\n",
        "        pair = random.choice(test_pairs)\n",
        "        reference = [pair[1].split(' ')]\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        candidate = output_sentence.split(' ')\n",
        "        i_score = sentence_bleu(reference, candidate)\n",
        "        score = score + i_score\n",
        "  avg_score = score/n\n",
        "\n",
        "  return(avg_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGLhsOdu3tb9",
        "outputId": "f7969095-654b-4553-aee4-6f2c414c6b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "base_score = BLEU_score(encoder1,attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lT4QRK84-kd",
        "outputId": "0ed372b5-cef6-4012-f6b9-bd2712b27e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tuned_score = BLEU_score(trained_encoder,trained_decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imhX4a8RgcRF",
        "outputId": "5152d19c-2e3b-4ae3-e3d1-ad5e5729f186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Number of Training Pairs : \", len(train_pairs))\n",
        "print(\"Number of Validation Pairs : \", len(val_pairs))\n",
        "print(\"Number of Test Pairs : \", len(test_pairs))\n",
        "print(\"Base Model Score : \", base_score*100)\n",
        "print(\"Tuned Model Score : \", tuned_score*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Pairs :  21266\n",
            "Number of Validation Pairs :  6076\n",
            "Number of Test Pairs :  3039\n",
            "Base Model Score :  22.28613835060829\n",
            "Tuned Model Score :  23.780408813884655\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}