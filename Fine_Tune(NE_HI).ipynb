{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune(NE-HI)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLfx2cobDIYN"
      },
      "source": [
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGoqKPZ4YJDe",
        "outputId": "a90012a6-2c26-4507-aed4-4ea6ca77ba72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install inltk\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import inltk\n",
        "from inltk.inltk import tokenize\n",
        "from inltk.inltk import setup\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: inltk in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from inltk) (0.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.5)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from inltk) (3.7.4.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from inltk) (3.2.2)\n",
            "Requirement already satisfied: aiohttp>=3.5.4 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.6.2)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from inltk) (7.352.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from inltk) (20.4)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from inltk) (1.3.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from inltk) (3.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from inltk) (4.6.3)\n",
            "Requirement already satisfied: async-timeout>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from inltk) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from inltk) (1.4.1)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from inltk) (0.2.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from inltk) (0.1.91)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.18.5)\n",
            "Requirement already satisfied: fastai==1.0.57 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.57)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from inltk) (2.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from inltk) (7.0.0)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from inltk) (2.2.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (2.4.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (19.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.7.4.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.1.0)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (4.7.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (1.12.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (1.24.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (0.6.1+cu101)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.7.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (49.1.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.1.0)\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2UbHQwXYTgt"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "#initialize Lang Class\n",
        "class Lang:\n",
        "   def __init__(self):\n",
        "       #initialize containers to hold the words and corresponding index\n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "       self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "#split a sentence into words and add it to the container\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "#If the word is not in the container, the word will be added to it, \n",
        "#else, update the word counter\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJQo1m3cYa27"
      },
      "source": [
        "def process_data():\n",
        "\n",
        "  sf = open('/content/drive/My Drive/data/bible-uedin.hi-ne.ne' , \"r\")\n",
        "  tf = open('/content/drive/My Drive/data/bible-uedin.hi-ne.hi' , \"r\")\n",
        "\n",
        "  source = Lang()\n",
        "  target = Lang()\n",
        "  pairs = []\n",
        "  count = 0\n",
        "  count2 = 0\n",
        "  for sent in sf:\n",
        "    sent = sent.strip()\n",
        "    source.addSentence(sent)\n",
        "    pairs.append(sent)\n",
        "    count = count+1\n",
        "\n",
        "  # print(count)\n",
        "\n",
        "  for sent in tf:\n",
        "    sent = sent.strip()\n",
        "    target.addSentence(sent.strip())\n",
        "    pairs.append(sent)\n",
        "    count2 = count2 + 1\n",
        "\n",
        "  # print(count2)\n",
        "\n",
        "  pairs_new = []\n",
        "\n",
        "  for i in range(count):\n",
        "    full = [pairs[i],pairs[i+count]]\n",
        "    pairs_new.append(full)\n",
        "\n",
        "\n",
        "\n",
        "  return source, target, pairs_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92bJScQFZ3Kz"
      },
      "source": [
        "# source, target, pairs = process_data()   JUST A CHECK TO SEE IF THE SIZE OF BOTH DATASET IS SAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRftbm5OC9Mp"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBdBN2bxdbC"
      },
      "source": [
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gUpG5q-DDop"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOZOdZTjdAWa"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PYQnQWvE0w5"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4xdfq9OdXqG"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tFsKzhQda3T"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBjkaSjjdgDg"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        # plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "    #     if iter % plot_every == 0:\n",
        "    #         plot_loss_avg = plot_loss_total / plot_every\n",
        "    #         plot_losses.append(plot_loss_avg)\n",
        "    #         plot_loss_total = 0\n",
        "\n",
        "    # showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLAP-AQedj8D"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(val_pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNDMkSjdx42",
        "outputId": "4308a7e3-7280-4ad8-bfd7-515280fe6b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "input_lang, output_lang, pairs = process_data()\n",
        "pairs = filterPairs(pairs)\n",
        "\n",
        "print(random.choice(pairs))\n",
        "\n",
        "print(len(pairs))\n",
        "x = len(pairs)\n",
        "\n",
        "trainSplit = int(x*0.7)\n",
        "valSplit = trainSplit + int(x*0.2)\n",
        "\n",
        "train_pairs = pairs[:trainSplit]\n",
        "val_pairs = pairs[trainSplit:valSplit]\n",
        "test_pairs = pairs[valSplit:]\n",
        "\n",
        "print(len(train_pairs))\n",
        "print(len(val_pairs))\n",
        "print(len(test_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['अब म बिन्ती गरिरहेछु केही खाऊ। यो तिमीहरूलाई बाँच्नको लागि आवश्यक हो। तिमीहरू कसैको पनि टाउकोको एउटा केश पनि नष्ट हुँदैन।”', 'इसलिये तुम्हें समझाता हूं; कि कुछ खा लो, जिस से तुम्हारा बचाव हो; क्योंकि तुम में से किसी के सिर पर एक बाल भी न गिरेगा।']\n",
            "30486\n",
            "21340\n",
            "6097\n",
            "3049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F5bEVeZzcXW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwctMktLdqQw",
        "outputId": "6adb2ab5-2a46-42c8-fb52-17c087e14e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 30000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 31s (- 44m 16s) (1000 3%) 6.0689\n",
            "3m 0s (- 42m 1s) (2000 6%) 5.6734\n",
            "4m 28s (- 40m 14s) (3000 10%) 5.5950\n",
            "5m 56s (- 38m 35s) (4000 13%) 5.5655\n",
            "7m 23s (- 36m 56s) (5000 16%) 5.4634\n",
            "8m 50s (- 35m 23s) (6000 20%) 5.3613\n",
            "10m 20s (- 33m 57s) (7000 23%) 5.3017\n",
            "11m 46s (- 32m 23s) (8000 26%) 5.1537\n",
            "13m 10s (- 30m 43s) (9000 30%) 5.0703\n",
            "14m 38s (- 29m 17s) (10000 33%) 5.2424\n",
            "16m 7s (- 27m 51s) (11000 36%) 5.1525\n",
            "17m 34s (- 26m 21s) (12000 40%) 5.2212\n",
            "19m 5s (- 24m 57s) (13000 43%) 5.3283\n",
            "20m 33s (- 23m 30s) (14000 46%) 5.2057\n",
            "22m 2s (- 22m 2s) (15000 50%) 5.1365\n",
            "23m 31s (- 20m 35s) (16000 53%) 5.1381\n",
            "25m 2s (- 19m 8s) (17000 56%) 5.1524\n",
            "26m 30s (- 17m 40s) (18000 60%) 5.0888\n",
            "27m 59s (- 16m 12s) (19000 63%) 5.1202\n",
            "29m 30s (- 14m 45s) (20000 66%) 5.0881\n",
            "31m 1s (- 13m 17s) (21000 70%) 5.0339\n",
            "32m 30s (- 11m 49s) (22000 73%) 4.9444\n",
            "33m 59s (- 10m 20s) (23000 76%) 4.9257\n",
            "35m 29s (- 8m 52s) (24000 80%) 4.9313\n",
            "36m 59s (- 7m 23s) (25000 83%) 4.8959\n",
            "38m 30s (- 5m 55s) (26000 86%) 4.9516\n",
            "40m 2s (- 4m 26s) (27000 90%) 4.9592\n",
            "41m 33s (- 2m 58s) (28000 93%) 5.0310\n",
            "43m 4s (- 1m 29s) (29000 96%) 4.8456\n",
            "44m 35s (- 0m 0s) (30000 100%) 4.8460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLSoH_89d5Eg",
        "outputId": "b9b27d78-dde2-4f48-aecb-d982303c236c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> भोलिपल्ट बिहान, येशू चाँडै उठनुभयो। उहाँले घर छोडनुभयो, विहान अंध्यारोमा नै येशू एकान्तमा जानूभयो अनि प्रार्थना गर्न लाग्नु भयो।\n",
            "= और भोर को दिन निकलने से बहुत पहिले, वह उठकर निकला, और एक जंगली स्थान में गया और वहां प्रार्थना करने लगा।\n",
            "< तब ने ने से के और और और और और और और और से और और के और <EOS>\n",
            "\n",
            "> यो दृष्टान्तको अर्थ के हो भनेः बीऊ परमेश्वरको वचन हो।\n",
            "= दृष्टान्त यह है; बीज तो परमेश्वर का वचन है।\n",
            "< क्या क्या से का है? <EOS>\n",
            "\n",
            "> परमप्रभु भन्नुहुन्छ, “तिमीहरुका पिता-पुर्खाहरु जस्तो नबन बितेका समयमा अगमवक्ताहरु तिनीहरुसित कुरा गर्थे। तिनीहरुले भने, ‘तिमीहरुले आफ्नो नराम्रो कु-कर्महरु छोडिदेऊ।’ तर तिमीहरुका पुर्खाहरुले मेरो एक शब्द पनि सुनेनन्।” परमप्रभुले यो कुरा भन्नुभयो।\n",
            "= अपने पुरखाओं के समान न बनो, उन से तो अगले भविष्यद्वक्ता यह पुकार पुकारकर कहते थे कि सेनाओं का यहोवा यों कहता है, अपने बुरे मार्गों से, और अपने बुरे कामों से फिरो; परन्तु उन्हों ने न तो सुना, और न मेरी ओर ध्यान दिया, यहोवा की यही वाणी है।\n",
            "< यहोवा यहोवा के कहा, यहोवा से यहोवा के के मैं के और के मैं के के मैं के और के <EOS>\n",
            "\n",
            "> तर येशूलाई सबै मानिसहरूले देख्न सकेनन्। जजसलाई परमश्वरले गवाहीको रूपमा छान्नु भएको थियो तिनीहरूले मात्र येशूको दर्शन पाए। हामी ती गवाहीहरू हौं उहाँ मृत्युबाट बौरे पछि हामीले येशूसंगै खाना-पिना गर्यौं।\n",
            "= उस को परमेश्वर ने तीसरे दिन जिलाया, और प्रगट भी कर दिया है।\n",
            "< तब वे ने से के और के और और के से और और हम के से <EOS>\n",
            "\n",
            "> त्यसपछि ती दुष्ट मानिसहरू सदा-सर्वदा दण्डित हुनको निम्ति जाने छन्। तर असल मानिसहरू चाँहिअनन्त जीवन 2प्राप्त गर्न जानेछन्।”\n",
            "= और यह अनन्त दण्ड भोगेंगे परन्तु धर्मी अनन्त जीवन में प्रवेश करेंगे।\n",
            "< वे लोग के के के और के के नहीं से <EOS>\n",
            "\n",
            "> त्यहाँ केही जातिहरु छन् जसले मेरो कुरा सुन्दैनन्। म तिनीहरुसित क्रोधित हुनेछु र म तिनीहरुसित बद्ला लिनेछु।”\n",
            "= और मैं अन्यजातियों से जो मेरा कहा नहीं मानतीं, क्रोध और जल जलाहट के साथ पलटा लूंगा।।\n",
            "< और मैं मेरे संग और और और और और और और और और और मैं और और और मैं और और और मैं ही और <EOS>\n",
            "\n",
            "> यसकारण म हजाएलको घरमा आगो लगाई दिनेछु अनि त्यो आगोले बेनहददका पर्खालहरु भस्म पार्नेछ।\n",
            "= इसलिये मैं हजाएल के राजभवन में आग जलाऊंगा, और उस से बेन्हदद के राजभवन भी भस्म हो जाएंगे।\n",
            "< मैं मैं ही मैं ही मैं ही मैं और मैं और मैं और मैं ही मैं ही मैं ही मैं ही और मैं ही <EOS>\n",
            "\n",
            "> म उसको मुखबाट रगत र उसको दाँतबाट घृणापूर्ण वस्तुहरू निकाल्नेेछु। बाँचेका कुनैपनि पलिश्तीहरू हाम्रो जतिको एक अंग बन्नेछन्। तिनीहरू यहूदाको एटा नयाँ कुल हुनेछन्। अक्रोनका मानिसहरू हाम्रा मानिसका एक भाग हुनेछन्, यबूसीहरू बनिए जस्तै।\n",
            "= मैं उसके मुंह में से आहेर का लोहू और घिनौनी वस्तुएं निकाल दूंगा, तब उन में से जो बचा रहेगा, वह हमारे परमेश्वर का जन होगा, और यहूदा में अधिपति सा होगा; और एक्रोन के लोग यबूसियों के समान बनेंगे।\n",
            "< और मैं के और और और और और के और और के और और के और और के <EOS>\n",
            "\n",
            "> केही समय पछि बाटै मलाई संसारमा मान्छेहरूले फेरि देख्नेछैन। तर तिमीहरूले मलाई देख्नेछौं। तिमीहरू बाँच्नेछौं किनभने म बाँच्छु।\n",
            "= और थोड़ी देर रह गई है कि संसार मुझे न देखेगा, परन्तु तुम मुझे देखोगे, इसलिये कि मैं जीवित हूं, तुम भी जीवित रहोगे।\n",
            "< मैं तुम के नहीं तो से मैं और मैं और मैं तुम ही के और मैं मैं ही मैं ही मैं ही मैं ही मैं ही मैं ही मैं ही मैं ही मैं ही मैं ही <EOS>\n",
            "\n",
            "> परमेश्वरको सत्य ज्ञान राख्नु पर्छ भन्ने कुरालाई मानिसहरूले महत्व दिएनन्। त मानिसहरूले नगर्नु पर्ने काम गरे। यसैले व्यर्थकै कामहरू गरोस् भनेर परमेश्वरले छाडिदिनु भयो।\n",
            "= और जब उन्हों ने परमेश्वर को पहिचानना न चाहा, इसलिये परमेश्वर ने भी उन्हें उन के निकम्मे मन पर छोड़ दिया; कि वे अनुचित काम करें।\n",
            "< वह ने ने से से उसके उसके और और उसके ने उसके और में से है। <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLvGEhf0j0C1"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
        "       super().__init__()\n",
        "      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "#encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "#use the encoder’s hidden layer as the decoder hidden\n",
        "       decoder_hidden = encoder_hidden.to(device)\n",
        "  \n",
        "#add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "\n",
        "#topk is used to get the top K value over a list\n",
        "#predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == EOS_token):\n",
        "               break\n",
        "\n",
        "       return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Piwtmuh7jDWh"
      },
      "source": [
        "model = Seq2Seq(encoder1, attn_decoder1, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3a93SLXj2gD",
        "outputId": "c1e8a14e-1ede-41d4-dd33-313d35e0e717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for name, params in model.named_children():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder\n",
            "decoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5faGpUDKlZwZ"
      },
      "source": [
        "for param in model.parameters():    \n",
        "    param.requires_grad = False\n",
        "\n",
        "trained_encoder = list(model.children())[0]\n",
        "trained_decoder = list(model.children())[1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6vf0E0ScfsV",
        "outputId": "a14e7580-4845-45f4-aac5-f06531d057b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder2 = trained_decoder\n",
        "\n",
        "\n",
        "trainIters(encoder2, attn_decoder2, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 8s (- 16m 5s) (1000 6%) 5.1840\n",
            "2m 17s (- 14m 56s) (2000 13%) 5.0214\n",
            "3m 27s (- 13m 50s) (3000 20%) 5.0823\n",
            "4m 36s (- 12m 40s) (4000 26%) 5.0752\n",
            "5m 44s (- 11m 28s) (5000 33%) 5.0554\n",
            "6m 51s (- 10m 17s) (6000 40%) 4.9794\n",
            "7m 59s (- 9m 8s) (7000 46%) 5.0815\n",
            "9m 8s (- 7m 59s) (8000 53%) 5.0330\n",
            "10m 16s (- 6m 51s) (9000 60%) 5.0714\n",
            "11m 26s (- 5m 43s) (10000 66%) 4.9687\n",
            "12m 35s (- 4m 34s) (11000 73%) 5.0261\n",
            "13m 45s (- 3m 26s) (12000 80%) 5.0319\n",
            "14m 55s (- 2m 17s) (13000 86%) 4.9985\n",
            "16m 4s (- 1m 8s) (14000 93%) 5.0323\n",
            "17m 14s (- 0m 0s) (15000 100%) 5.0114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul6xiMFA6YA6",
        "outputId": "67b0eedb-547c-40c1-d48a-9796e723b912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "new_model = Seq2Seq(encoder2, attn_decoder2, device).to(device)\n",
        "\n",
        "for param in new_model.parameters():    \n",
        "    param.requires_grad = True\n",
        "\n",
        "trained_encoder = list(new_model.children())[0]\n",
        "trained_decoder = list(new_model.children())[1] \n",
        "\n",
        "\n",
        "trainIters(trained_encoder, trained_decoder, 15000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 34s (- 22m 1s) (1000 6%) 5.0558\n",
            "3m 3s (- 19m 50s) (2000 13%) 4.8134\n",
            "4m 35s (- 18m 21s) (3000 20%) 4.8986\n",
            "6m 6s (- 16m 46s) (4000 26%) 4.8820\n",
            "7m 37s (- 15m 15s) (5000 33%) 4.8448\n",
            "9m 9s (- 13m 44s) (6000 40%) 4.9377\n",
            "10m 40s (- 12m 11s) (7000 46%) 4.8153\n",
            "12m 10s (- 10m 39s) (8000 53%) 4.9169\n",
            "13m 42s (- 9m 8s) (9000 60%) 4.8525\n",
            "15m 17s (- 7m 38s) (10000 66%) 4.8857\n",
            "16m 49s (- 6m 6s) (11000 73%) 4.7492\n",
            "18m 19s (- 4m 34s) (12000 80%) 4.8747\n",
            "19m 52s (- 3m 3s) (13000 86%) 4.8659\n",
            "21m 24s (- 1m 31s) (14000 93%) 4.8442\n",
            "22m 58s (- 0m 0s) (15000 100%) 4.8133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WCaEOcIuVmV"
      },
      "source": [
        "def BLEU_score(encoder, decoder, n = len(test_pairs)):\n",
        "  score = 0\n",
        "  for i in range(n):\n",
        "        pair = random.choice(test_pairs)\n",
        "        reference = [pair[1].split(' ')]\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        candidate = output_sentence.split(' ')\n",
        "        i_score = sentence_bleu(reference, candidate)\n",
        "        score = score + i_score\n",
        "  avg_score = score/n\n",
        "\n",
        "  return(avg_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGLhsOdu3tb9",
        "outputId": "eebab4d1-d2ea-49bd-a60c-88f89938064d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "base_score = BLEU_score(encoder1,attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lT4QRK84-kd",
        "outputId": "7ff04c68-98f9-4101-823b-80508802bda3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tuned_score = BLEU_score(trained_encoder,trained_decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imhX4a8RgcRF",
        "outputId": "610e2bc3-5ce4-4c08-e214-eecc227a9945",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Number of Training Pairs : \", len(train_pairs))\n",
        "print(\"Number of Validation Pairs : \", len(val_pairs))\n",
        "print(\"Number of Test Pairs : \", len(test_pairs))\n",
        "print(\"Base Model Score : \", base_score*100)\n",
        "print(\"Tuned Model Score : \", tuned_score*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Pairs :  21340\n",
            "Number of Validation Pairs :  6097\n",
            "Number of Test Pairs :  3049\n",
            "Base Model Score :  36.34311005628223\n",
            "Tuned Model Score :  37.338164343879335\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}